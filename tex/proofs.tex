\section{PROOFS}\label{sec:proofs}

\subsection{Proof of \Cref{thm:sl1-directional-derivative}}
\label{app:proof_directional_derivative}

Let \(c^{\setminus k}\) be the set containing all elements of $c$ except the $k$-th one: $c^{\setminus k} =  \{c_1, \ldots c_{k-1}, c_{k+1}, \ldots, c_m \}$.

From the observations in \Cref{rem:permutation_C_z},
we have the following cases to consider: \(|z| \in c^{\setminus k}\),
\(|z| = 0\), and \(|z| \notin \{0\} \cup c^{\setminus k}\).


\paragraph{Case 1}
Let us first consider the case $z \neq 0$, $|z| \neq c_i$ for all $i \neq k$.
Since \(C(z + \delta h) = C(z) = \cC_k\) and $\sign(z + \delta h) = \sign(z)$ for $h$ small enough,
\begin{align}
  H(z + \delta h) - H(z)
   & = \sum_{j =1}^p |\beta(z + \delta h)_j| \lambda_{(j)^-_{z + \delta h}}
  - \sum_{j=1}^p |\beta(z)_j| \lambda_{(j)^-_z} \nonumber                                       \\
   & = \sum_{j =1}^p (|\beta(z + \delta h)_j| - |\beta(z)_j|) \lambda_{(j)^-_z} \nonumber       \\
   & = \sum_{j =1}^p (|\beta(z + \delta h)_j| - |\beta(z)_j|) \lambda_{(j)^-_z} \nonumber       \\
   & = \sum_{j \in C(z)}^p (|\beta(z + \delta h)_j| - |\beta(z)_j|) \lambda_{(j)^-_z} \nonumber \\
   & = \sum_{j \in C(z)} \sign(\beta(z)_j) (z + \delta h - z) \lambda_{(j)^-_z} \nonumber       \\
   & = \sum_{j \in C(z)} \sign(z) \delta h  \lambda_{(j)^-_z} \nonumber                         \\
   & = \sum_{j \in \cC_k} \sign(z) \delta h  \lambda_{(j)^-_z} \, .
\end{align}

\paragraph{Case 2}
Then if  $z \neq 0$ and $|z|$ is equal to one of the $c_i$'s, $i \neq k$,  one has $C(z) = \cC_k \cup \cC_i$, $C(z + \delta h) = \cC_k$, and $\sign(z + \delta h) = \sign(z)$ for $h$ small enough.
Thus
\begin{align}
  H(z + \delta h) - H(z)
   & = \sum_{j =1}^p |\beta(z + \delta h)_j| \lambda_{(j)^-_{z + \delta h}}
  - \sum_{i=1}^p |\beta(z)_j| \lambda_{(j)^-_z}  \nonumber                                         \\
   & = \sum_{j \in \cC_k \cup \cC_i} \left( |\beta(z + \delta h)_j| \lambda_{(j)^-_{z + \delta h}}
  - |\beta(z)_j| \lambda_{(j)^-_z} \right)  \nonumber                                              \\
   & = \sum_{j \in \cC_k} \left( c_i + \delta h \right) \lambda_{(i)^-_{z + \delta h}}
  - c_i \lambda_{(i)^-_z}
  + \sum_{j \in \cC_i} \left( c_i \lambda_{(j)^-_{z + \delta h}}
    - c_i \lambda_{(i)^-_z} \right) \, .
\end{align}
\mm{to conclude we need to mention there is an ambiguity in terms of permutation (the permutation reordering $\beta(z)$ is not unique, we can swap $\cC_i$ and $\cC_k$, but it does not change the value of the sum, so we can pick $(i)_z = (i)_{z + \delta_h}$ and conclude) as above.}

\paragraph{Case 3} Finally let us treat the case $z = 0$.
If $c_m = 0$ then the proof proceeds as in case 2, with the exception that $|\beta(z + \delta h)| = h$ and so the result is just:
\begin{align}
  H(z + \delta h) - H(z)
   & = h \sum_{j \in \cC_k} \lambda_{(i)^-_{z + \delta h}} \, .
\end{align}
If $c_m \neq 0$, then the computation proceeds exactly as in case 1.

\subsection{Proof of \Cref{thm:thresholding-operator}}

Recall that \(G(z) : \mathbb{R} \to \mathbb{R}\) is a convex,
continuous piecewise-differentiable function with breakpoints whenever \(|z| =
c_i^{\setminus k}\) or \(z = 0\). Let \(\gamma = c_k \norm{\tilde{x}}^2+ \tilde x^Tr\)
and \(\omega = \norm{\tilde{x}}^2\) and note that the optimality criterion for
\eqref{pb:cluster-problem} is
\[
  \delta(\omega z - \gamma) + H'(z; \delta) \geq 0, \quad
  \forall \delta \in \{-1, 1\},
\]
which is equivalent to
\begin{equation}
  \label{eq:optimality-inequality}
  \omega z - H'(z; -1) \leq \gamma \leq \omega z + H'(z; 1).
\end{equation}
We now proceed to show that there is a solution \(z^* \in \argmin_{z \in
  \mathbb{R}} H(z)\) for every interval over \(\gamma \in \mathbb{R}\).

First, assume that the first case in the definition of \(T\) holds
and note that this is equivalent to \eqref{eq:optimality-inequality} with \(z
= 0\) since \(C({\varepsilon_c}) = C(-{\varepsilon_c})\) and
\(\lambda_{(j)^-_{-{\varepsilon_c}}} = \lambda_{(j)^-_{{\varepsilon_c}}}\).
This is sufficient for \(z^* = 0\).

Next, assume that the second case holds and observe that this is equivalent
to \eqref{eq:optimality-inequality} with
\(z = c_i^{\setminus k}\), since
\(C(c_i + {\varepsilon_c}) = C(-c_i - {\varepsilon_c})\) and
\(C(-c_i + {\varepsilon_c}) = C(c_i - {\varepsilon_c})\). Thus \(z^* =
\sign(\gamma)c_i^{\setminus k}\).

For the third case, we have
\[
  \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}}
  =
  \smashoperator[r]{\sum_{j \in C(c_{i-1} - {\varepsilon_c})}} \lambda_{(j)^-_{c_{i-1} - {\varepsilon_c}}}
\]
and therefore \eqref{eq:optimality-inequality} is equivalent to
\[
  c_i < \frac{1}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}} \bigg) < c_{i -1}.
\]
Now let
\begin{equation}
  \label{eq:differentiable-solution}
  z^* = \frac{\sign(\gamma)}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}} \bigg)
\end{equation}
and note that \(|z^*| \in \big(c_i^{\setminus k}, c_{i-1}^{\setminus k}\big)\) and hence
\[
  \frac{1}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}} \bigg)
  =
  \frac{1}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(z^*)}} \lambda_{(j)^-_{z^*}} \bigg).
\]
Furthermore, since \(G\) is differentiable in \(\big(c_i^{\setminus k}, c_{i-1}^{\setminus k}\big)\), we have
\[
  \frac{\partial}{\partial z} G(z) \Big|_{z = z^*}
  = \omega z^* - \gamma + \sign(z^*) \smashoperator{\sum_{j \in C(z^*)}} \lambda_{(j)^-_{z^*}} = 0,
\]
and therefore \eqref{eq:differentiable-solution} must be the solution.

The solution for the last case follows using reasoning analogous to that of the
third case.

\subsection{Proof of \Cref{lem:convergence}}
We prove the results by establishing two mirror Lemmas for coordinate descent as \textcite[Lemmas 2.2,2.3]{beck2009} is for proximal gradient descent. After this one can just apply the proof of \textcite[Theorem 3.1]{beck2009} directly.
First we show the corresponding \textcite[Lemmas 2.1]{beck2009} (we follow the notation of the that paper to simplify comparison). We prove the result for a more general function that only LS in \eqref{pb:slope} that is for any convex function $f$ that is Lipschitz continuous gradient with Lipschitz constant $L(f)$ 
\begin{lemma}
% For any ${y} \in \mathbb{R}^d$, one has ${z}=p_{CD}(\mv{y})$ where $p_{CD}$ is the minimizer of  $G$ in \eqref{pb:cluster-problem}. Then there exists a $\gamma({y}) \in \partial J({z})$ such that
For any ${y} \in \mathbb{R}^d$, one has ${z}=p_{CD}(y)$ where $p_{CD}$ is the minimizer of  $G$ in \eqref{pb:cluster-problem}. Then there exists a $\gamma({y}) \in \partial J({z})$ such that
$$
\langle {z}- {y},\nabla f({y}) + \gamma(y) \rangle + L(f) ||  {z}-{y}||^2  \leq 0.
$$
\end{lemma}
\begin{proof}
Since solving $G$ is equivalent to minimizing $P$ in the direction ${d} = {z}- {y}$ it follows that that there exists a $\gamma({y}) \in \partial J({z})$ such that
$$
\langle {z}- {y},\nabla f({z}) + \gamma(y) \rangle  = 0.
$$
Now since $f$ is Lipschitz continuous it follows that 
\begin{align*}
0 \leq f(z)  &\leq f(y) +   \langle z-y,\nabla f({y})  \rangle + \frac12 L(f) ||  {z}-{y}||^2  \\
 & \leq   \langle y-z,\nabla f({z})  \rangle + \langle z-y,\nabla f({y})  \rangle + L(f) ||  {z}-{y}||^2,
\end{align*}
hence 

$
\langle {z}- {y},\nabla f({z}) \rangle \leq \langle {z}- {y},\nabla f({y}) \rangle + L(f) ||  {z}-{y}||^2  $ and the lemma follows.
\end{proof}

We prove the results by showing that a step of coordinate descent has the same convergence results as proximal gradient descent. Then after  
\mathurin{I'm not sure this is so straightforwrd}

From \cref{thm:thresholding-operator}, we know that lines 9--14 in \cref{alg:hybrid} correspond to minimizing \(G(z)\) for a given \(\beta \coloneqq \beta^{(t + k / |\mathcal{C}|)}\), and therefore that
\[
  G\big(\beta^{(t + (k - 1) / |\mathcal{C}|)}\big) \leq G\big(\beta^{(t + k / |\mathcal{C}|)}\big)
\]
since \(G(z) = P\big(\beta(z)\big)\).

Moreover, for a sequence of iterates of the proximal gradient descent step,
\(\beta^{(v)}, \beta^{(2v)}, \dots, \beta^{(\lfloor k / v \rfloor)}\),
we know from~\textcite[Theorem 3.1]{beck2009} that it holds that
\[
  P(\beta^{(\lfloor k / v \rfloor)}) - P(\beta^*)
  \leq \frac{\norm{X}_2^2 \norm{\beta^{(0)} - \beta^*}^2}{2\lfloor k / v \rfloor}.
\]
Combining this and the result from the previous paragraph yields the desired
result.



\subsection{Partial smoothness of the sorted $\ell_1$ norm}
\label{app:sec:partly_smooth}
In this section, we prove that the sorted $\ell_1$ norm $J$ is partly smooth~\parencite{lewis2003active}.
This allows to apply result about the structure identification of the proximal gradient algorithm.

\begin{definition}
  Let $J$ be a proper closed convex function ad $x$ a point of its domain such that $\partial J(x) \neq \emptyset$.
  $J$ is said to be partly smooth at $x$ relative to a set $\cM$ containing x if:
  \begin{enumerate}
    \item $\cM$ is a $C^2$-manifold around $x$ and $J$ restricted to $\cM$ is $C^2$ around $x$
    \item The tangent space of $\cM$ at $x$ is the orthogonal of the linear hull of $\partial J(x)$.
    \item $\partial J$ is continuous at $x$ relative to $\cM$
  \end{enumerate}
\end{definition}

\begin{proposition}
  The sorted $\ell_1$ norm is partly smooth at any point of $\bbR^d$.
\end{proposition}

\begin{proof}
  % First, $J$ is convex and has full domain, so its subdifferential is always non-empty.
  % Let $x \in \bbR^d$.

  First, we consider the case where $| x_j| >0$ for all $j\in\bbR^d$.
  Let $m$ be the number of clusters of $x$ and $\cC_1, \ldots, \cC_m$ be those clusters, and let $c_1 > \ldots > c_m > 0$ be the  value of  $\lvert x \rvert$ on the clusters.

  We define $\varepsilon_c$ as in \Cref{eq:epsilon-c} and 
  let $\cB = \{u \in \bbR^d: \lVert u - x \rVert_\infty < \varepsilon_c / 2\}$.
  
  We will show that $J$ is partly smooth at $x$ relative to the subspace
  \begin{equation}
    \cM = \{ u \in \bbR^d : (\exists c' \in \bbR^m, \forall i \neq j \in [m], c'_i \neq c'_j) \& (\forall k \in [m], \forall j \in \cC_k, u_j = \sign(x_j) c'_k) \} \cap \cB \, .
  \end{equation}
  Moreover, let $v_k \in \bbR^d$ for $k \in [m]$  be equal to $\sign x_{\cC_k}$ on $\cC_k$ and to 0 outside, such that $x = \sum_{k=1}^m c_k v_k$.
  The set $\cM$ can then be rewritten as $\cM = \Span(v_1, \ldots v_m) \cap \cB$.
  One inclusion is trivial.
  For the other, let $u \in \Span(v_1, \ldots v_m) \cap \cB$.
  Then there exists $c' \in \bbR^k$, $u = \sum_{k=1}^m c'_k v_k$.
  Suppose that there exist $k \neq k'$ such that $c'_k = c'_{k'}$.
  Then since $\lVert x - u \rVert_\infty = \max_k |c_k - c'_k|$ and $|c_k - c_{k'}| > \varepsilon_c$, one has:
  \begin{align*}
     \varepsilon_c < |c_k - c_{k'}|
     &= |c_k - c'_k + c'_{k'} - c_{k'}| \\
     &\leq |c_k - c'_k| + |c'_{k'} - c_{k'}| \\
     &\leq 2 \lVert x - u \rVert_\infty \\
     &\leq  \varepsilon_c \, .
  \end{align*}

  This shows that clusters of any $u \in \cM$ are equal to clusters of $x$.
  In addition, the tangent space of $\cM$ at $x$ is $\Span(v_1, \hdots, v_m)$. 
  
  
  % $\{ u \in \bbR^d : (\exists c' \in \bbR^m, \forall i \neq j \in [d], c'_i \neq c'_j) \& (\forall k \in [m], \forall j \in \cC_k, |u_j| = c'_k) \}$.

  
%   $k(j):[d] \rightarrow [m]$  such that $j \in \cC_{k(j}$.
%  \begin{equation}
%     \cM = \{ u \in \bbR^d : u_j = \sign(x_j) c'_{k(j)}\mbox{ where }  c' \in \bbR^m, \forall i \neq l , c'_i \neq c'_l \} \cap \cB \, .
%   \end{equation}

\begin{enumerate}
  \item The set $\cM$ is then the intersection of a linear subspace and an open ball hence it is a $\cC^2$ manifold. 
  Following the observation of the fact that the clusters of any $u\in\cM$ are the same than the clusters of $x$, we have that 
  \begin{align}{}
    J(u) = \sum_{k=1}^m \left( \sum_{j \in \cC_k} \lambda_j \right)c_k' \enspace ,
  \end{align}
  hence $J$ is linear on $\cM$ thus $\cC^2$. 
  \item The subdifferential of $J$ at $x$ is 
  \begin{align}
    \partial J(x) = 
  \end{align}
  \klopfe{Compute what is the linear hull of subdifferential restricted to one cluster}
  \item The subdifferential of $J$ is a constant set locally around $x$ along $\cM$ which shows that it is continuous at $x$ relative to $\cM$. 
\end{enumerate}

% We will now establish that $J$ is linear on $\mathcal{M}$.
%   let $v_k \in \bbR^d$ for $k \in [m]$  be equal to $\sign x_{\cC_k}$ on $\cC_k$ and to 0 outside, such that $x = \sum_{k=1}^m c_k v_k$.

%   Note that for $u =\sum_{k=1}^m c'_k v_k\in \Span(v_1, \ldots v_m) \cap \cB$ the function $J(u) = \sum_{k=1}^m \left( \sum_{j \in \cC_k} \lambda_j \right)c_k'$ which is clearly linear hence 1 and 3 holds.




  $J$ is linear hence smooth on $\cM$, its gradient is TODO $\sum_k (\sum_{j \in \cC_k} \lambda_j) v_k$. \mm{and this is orthogonal to $\cM$ because??? it cannot be, it's in $\cM$}

  If one cluster of $x$ vanishes ($c_m = 0)$, the same reasoning holds wih the manifold $cM \cap \{u \in \bbR^d: u_{\cC_m} = 0\}$ aka $c'_m = 0$).
\end{proof}

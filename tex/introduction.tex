%!TEX root = ./slopecd.tex
\section{Introduction}\label{sec:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper we present a novel numerical algorithm for Sorted L-One Penalized
Estimation (SLOPE)~\cite{bogdan2013, bogdan2015}, defined as
\begin{problem}
  \label{pb:slope}
    \min_{\beta \in \mathbb{R}^p}
        P(\beta) = L(\beta) + J(\beta)
\end{problem}
\mm{TODO change}
where we take \(L\) to be smooth and twice differentiable and
% \mm{to discuss: $L(X\beta)$ doable? + some computation are sepcific to  the quadratic case}
\begin{equation}
  \label{eq:sorted-l1-norm}
  J(\beta) = \sum_{j=1}^p \lambda_j|\beta_{(j)}|
\end{equation}
is the \emph{sorted \(\ell_1\) norm}, defined such that
\[
  |\beta_{(1)}| \geq |\beta_{(2)}| \geq \cdots \geq |\beta_{(p)}|,
\]
and \(\lambda\) is a fixed non-increasing and non-negative
sequence.

The sorted $\ell_1$ norm is a sparsity-enforcing penalty  that has become increasingly popular due to
several appealing properties, such as its ability to control false discovery
rate~\cite{bogdan2015, kos2020}, cluster coefficients~\cite{figueiredo2016,
schneider2020a}, and recover sparsity and ordering patterns in the
solution~\cite{bogdan2022}. Unlike other competing sparse regularization methods such
as MCP~\cite{zhang2010} and SCAD~\cite{fan2001}, SLOPE is also a convex
problem~\cite{bogdan2015}.

In spite of the availability of predictor screening rules~\cite{elvira2022,
larsson2020c}, which help speed up SLOPE in the high-dimensional regime,
current state-of-the-art algorithms for SLOPE perform poorly in comparison to
those of other, more established, penalization methods such as the lasso
(\(\ell_1\)-norm regularization) and ridge regression (\(\ell_2\)-norm
regularization). As a small illustration of this issue, we compared the
speed at which the SLOPE and glmnet packages fit a complete regularization
path for the bcTCGA data set. SLOPE takes x seconds to fit the full path,
whilst glmnet requires only y seconds. \mm{put result here and details in experiment section}




This lackluster performance has hampered the applicability of SLOPE to many
real-world applications. % particularly because users often need to solve SLOPE
%repeatedly when tuning hyper-parameters, for instance in
%cross-validation, or when fitting SLOPE as part of Adaptive Bayesian
% SLOPE~\cite{jiang2022}.

A major reason for why algorithms for solving $\ell_1$-, MCP-, or SCAD-regularized problems enjoy better
performance is that they minimize their objectives using coordinate
descent~\cite{breheny2011, friedman2010}.
On the other hand, current SLOPE solvers rely on proximal gradient descent
algorithms such as FISTA~\cite{beck2009} and the
alternating direction method of multipliers method (ADMM)~\cite{boyd2010}, that in ML scenario is much slower than coordinate descent \cite{moreau2022benchopt}.
% which are both used in the current version of the SLOPE package.
% \jl{Add benchopt reference.}
% \mm{For which point exactly? I did not find a ref for your SLOPE R package, what's the good way to cite it? There also is PyOWL https://github.com/vene/pyowl}
\
Applying coordinate descent to SLOPE is, however, not quite straightforward
because convergence guarantees for coordinate descent require the
objective to be separable, which, as we can see from \eqref{eq:sorted-l1-norm}
is not the case for SLOPE.

% The resulting solution \(\hat{\beta}\) has the property that it can cluster the
% coefficients by their magnitudes, such that \(\mathcal{C}=
% \{j: |\beta_j|=c\}\) for some \(c\).

In this article we address this problem by introducing a new, highly effective
algorithm for SLOPE based on a hybrid proximal gradient and coordinate descent
scheme. Our method features convergence guarantees and reduces the time
required to fit SLOPE by orders of magnitude in our empirical experiments.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.62]{illustration_solvers}
  \caption{Illustration of proposed solver. \mathurin{0) run it on the full problem 1) add $\beta_1$ xlabel, $\beta_2$ ylabel 2) set aspect ratio to 1; 3) add pure CD on the left }}
  \label{fig:illustration-solver}
\end{figure}

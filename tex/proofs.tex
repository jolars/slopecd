\section{PROOFS}\label{sec:proofs}

\subsection{Proof of \Cref{thm:sl1-directional-derivative}}
\label{app:proof_directional_derivative}

Let \(c^{\setminus k}\) be the set containing all elements of $c$ except the $k$-th one: $c^{\setminus k} =  \{c_1, \ldots c_{k-1}, c_{k+1}, \ldots, c_m \}$.

From the observations in \Cref{rem:permutation_C_z},
we have the following cases to consider: \(|z| \in c^{\setminus k}\),
\(|z| = 0\), and \(|z| \notin \{0\} \cup c^{\setminus k}\).


\paragraph{Case 1}
Let us first consider the case $z \neq 0$, $|z| \neq c_i$ for all $i \neq k$.
Since \(C(z + \delta h) = C(z) = \cC_k\) and $\sign(z + \delta h) = \sign(z)$ for $h$ small enough,
\begin{align}
  H(z + \delta h) - H(z)
   & = \sum_{j =1}^p |\beta(z + \delta h)_j| \lambda_{(j)^-_{z + \delta h}}
  - \sum_{j=1}^p |\beta(z)_j| \lambda_{(j)^-_z} \nonumber                                       \\
   & = \sum_{j =1}^p (|\beta(z + \delta h)_j| - |\beta(z)_j|) \lambda_{(j)^-_z} \nonumber       \\
   & = \sum_{j =1}^p (|\beta(z + \delta h)_j| - |\beta(z)_j|) \lambda_{(j)^-_z} \nonumber       \\
   & = \sum_{j \in C(z)}^p (|\beta(z + \delta h)_j| - |\beta(z)_j|) \lambda_{(j)^-_z} \nonumber \\
   & = \sum_{j \in C(z)} \sign(\beta(z)_j) (z + \delta h - z) \lambda_{(j)^-_z} \nonumber       \\
   & = \sum_{j \in C(z)} \sign(z) \delta h  \lambda_{(j)^-_z} \nonumber                         \\
   & = \sum_{j \in \cC_k} \sign(z) \delta h  \lambda_{(j)^-_z} \, .
\end{align}

\paragraph{Case 2}
Then if  $z \neq 0$ and $|z|$ is equal to one of the $c_i$'s, $i \neq k$,  one has $C(z) = \cC_k \cup \cC_i$, $C(z + \delta h) = \cC_k$, and $\sign(z + \delta h) = \sign(z)$ for $h$ small enough.
Thus
\begin{align}
  H(z + \delta h) - H(z)
   & = \sum_{j =1}^p |\beta(z + \delta h)_j| \lambda_{(j)^-_{z + \delta h}}
  - \sum_{i=1}^p |\beta(z)_j| \lambda_{(j)^-_z}  \nonumber                                         \\
   & = \sum_{j \in \cC_k \cup \cC_i} \left( |\beta(z + \delta h)_j| \lambda_{(j)^-_{z + \delta h}}
  - |\beta(z)_j| \lambda_{(j)^-_z} \right)  \nonumber                                              \\
   & = \sum_{j \in \cC_k} \left( c_i + \delta h \right) \lambda_{(i)^-_{z + \delta h}}
  - c_i \lambda_{(i)^-_z}
  + \sum_{j \in \cC_i} \left( c_i \lambda_{(j)^-_{z + \delta h}}
    - c_i \lambda_{(i)^-_z} \right) \, .
\end{align}
\mm{to conclude we need to mention there is an ambiguity in terms of permutation (the permutation reordering $\beta(z)$ is not unique, we can swap $\cC_i$ and $\cC_k$, but it does not change the value of the sum, so we can pick $(i)_z = (i)_{z + \delta_h}$ and conclude) as above.}

\paragraph{Case 3} Finally let us treat the case $z = 0$.
If $c_m = 0$ then the proof proceeds as in case 2, with the exception that $|\beta(z + \delta h)| = h$ and so the result is just:
\begin{align}
  H(z + \delta h) - H(z)
   & = h \sum_{j \in \cC_k} \lambda_{(i)^-_{z + \delta h}} \, .
\end{align}
If $c_m \neq 0$, then the computation proceeds exactly as in case 1.

\subsection{Proof of \Cref{thm:thresholding-operator}}

Recall that \(G(z) : \mathbb{R} \to \mathbb{R}\) is a convex,
continuous piecewise-differentiable function with breakpoints whenever \(|z| =
c_i^{\setminus k}\) or \(z = 0\). Let \(\gamma = c_k \norm{\tilde{x}}^2+ \tilde x^Tr\)
and \(\omega = \norm{\tilde{x}}^2\) and note that the optimality criterion for
\eqref{pb:cluster-problem} is
\[
  \delta(\omega z - \gamma) + H'(z; \delta) \geq 0, \quad
  \forall \delta \in \{-1, 1\},
\]
which is equivalent to
\begin{equation}
  \label{eq:optimality-inequality}
  \omega z - H'(z; -1) \leq \gamma \leq \omega z + H'(z; 1).
\end{equation}
We now proceed to show that there is a solution \(z^* \in \argmin_{z \in
  \mathbb{R}} H(z)\) for every interval over \(\gamma \in \mathbb{R}\).

First, assume that the first case in the definition of \(T\) holds
and note that this is equivalent to \eqref{eq:optimality-inequality} with \(z
= 0\) since \(C({\varepsilon_c}) = C(-{\varepsilon_c})\) and
\(\lambda_{(j)^-_{-{\varepsilon_c}}} = \lambda_{(j)^-_{{\varepsilon_c}}}\).
This is sufficient for \(z^* = 0\).

Next, assume that the second case holds and observe that this is equivalent
to \eqref{eq:optimality-inequality} with
\(z = c_i^{\setminus k}\), since
\(C(c_i + {\varepsilon_c}) = C(-c_i - {\varepsilon_c})\) and
\(C(-c_i + {\varepsilon_c}) = C(c_i - {\varepsilon_c})\). Thus \(z^* =
\sign(\gamma)c_i^{\setminus k}\).

For the third case, we have
\[
  \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}}
  =
  \smashoperator[r]{\sum_{j \in C(c_{i-1} - {\varepsilon_c})}} \lambda_{(j)^-_{c_{i-1} - {\varepsilon_c}}}
\]
and therefore \eqref{eq:optimality-inequality} is equivalent to
\[
  c_i < \frac{1}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}} \bigg) < c_{i -1}.
\]
Now let
\begin{equation}
  \label{eq:differentiable-solution}
  z^* = \frac{\sign(\gamma)}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}} \bigg)
\end{equation}
and note that \(|z^*| \in \big(c_i^{\setminus k}, c_{i-1}^{\setminus k}\big)\) and hence
\[
  \frac{1}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}} \bigg)
  =
  \frac{1}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(z^*)}} \lambda_{(j)^-_{z^*}} \bigg).
\]
Furthermore, since \(G\) is differentiable in \(\big(c_i^{\setminus k}, c_{i-1}^{\setminus k}\big)\), we have
\[
  \frac{\partial}{\partial z} G(z) \Big|_{z = z^*}
  = \omega z^* - \gamma + \sign(z^*) \smashoperator{\sum_{j \in C(z^*)}} \lambda_{(j)^-_{z^*}} = 0,
\]
and therefore \eqref{eq:differentiable-solution} must be the solution.

The solution for the last case follows using reasoning analogous to that of the
third case.

\subsection{Proof of \Cref{lem:convergence}}

Let $\beta^*$ denote the stationary point of $F$.
If we denote the $t$th iteration of \Cref{alg:hybrid} as $T_t$, that is, $\beta^{(t+1)} = T_t(\beta^{t})$. Then by definition
$$
T_t(\beta)=\begin{cases}
T_{PGD}(\beta) & \mbox{if } t\bmod v=0, \\
T_{CD}(\beta) & \mbox{else,}  \\
\end{cases}
$$
where $T_{PGD}$ is an iteration of proximal gradient descent and $T_{CD}$ is an iteration of coordinate descent. Clearly $P(T_{CD}(\beta)) \leq P(\beta)$. Now note that
$$
T_{PGD}(\beta)= \argmin_{\hat{\beta}} \left( H(\beta,\hat{\beta}) = f(\beta) + \langle \nabla f(\beta),\hat{\beta} - \beta ) + \frac{L(f)}{2} ||\beta-\hat{\beta}  ||^2+ J(\beta) \right)
$$ 
where $f(\beta) = \frac{1}{2} \norm{y - X \beta}^2$ and $L(f)$ is the Lipschitz constant. Note that by strong convexity that $H(\beta,T_{PGD}(\beta)) \leq F(T_{PGD}(\beta))$.
Then we can apply Lemma 3 in \textcite{richtarik2014}
to get
$$
P(T_{PGD}(\beta)) - P(\beta^*)  \leq 
\begin{cases}
\left(1 - \frac{P(\beta) - P(\beta^*)}{R^2} \right) \left( P(\beta) - P(\beta^*)\right) & \mbox{if } \left( P(\beta) - P(\beta^*) \right)\leq R^2, \\
\frac{1}{2}\left( P(\beta) - P(\beta^*)\right) & \mbox{else,}
\end{cases}
$$
where $R^2=||\beta- \beta^*||_L$. To get the convergence rate apply proof of Theorem 1 in \textcite{richtarik2014}.

\subsection{Partial smoothness of the sorted $\ell_1$ norm}
\label{app:sec:partly_smooth}
In this section, we prove that the sorted $\ell_1$ norm $J$ is partly smooth~\parencite{lewis2002a}.
This allows to apply result about the structure identification of the proximal gradient algorithm.

\begin{definition}
  Let $J$ be a proper closed convex function ad $x$ a point of its domain such that $\partial J(x) \neq \emptyset$.
  $J$ is said to be partly smooth at $x$ relative to a set $\cM$ containing x if:
  \begin{enumerate}
    \item $\cM$ is a $C^2$-manifold around $x$ and $J$ restricted to $\cM$ is $C^2$ around $x$
    \item The tangent space of $\cM$ at $x$ is the orthogonal of the linear hull of $\partial J(x)$.
    \item $\partial J$ is continuous at $x$ relative to $\cM$
  \end{enumerate}
\end{definition}

\begin{proposition}
  The sorted $\ell_1$ norm is partly smooth at any point of $\bbR^d$.
\end{proposition}

\begin{proof}
  % First, $J$ is convex and has full domain, so its subdifferential is always non-empty.
  % Let $x \in \bbR^d$.

  First, we consider the case where $| x_j| >0$ for all $j\in\bbR^d$.
  Let $m$ be the number of clusters of $x$ and $\cC_1, \ldots, \cC_m$ be those clusters, and let $c_1 > \ldots > c_m > 0$ be the  value of  $\lvert x \rvert$ on the clusters.

  We define $\varepsilon_c$ as in \Cref{eq:epsilon-c} and 
  let $\cB = \{u \in \bbR^d: \lVert u - x \rVert_\infty < \varepsilon_c / 2\}$.
  
  We will show that $J$ is partly smooth at $x$ relative to the subspace
  \begin{equation}
    \cM = \{ u \in \bbR^d : (\exists c' \in \bbR^m, \forall i \neq j \in [m], c'_i \neq c'_j) \& (\forall k \in [m], \forall j \in \cC_k, u_j = \sign(x_j) c'_k) \} \cap \cB \, .
  \end{equation}
  Moreover, let $v_k \in \bbR^d$ for $k \in [m]$  be equal to $\sign x_{\cC_k}$ on $\cC_k$ and to 0 outside, such that $x = \sum_{k=1}^m c_k v_k$.
  The set $\cM$ can then be rewritten as $\cM = \Span(v_1, \ldots v_m) \cap \cB$.
  One inclusion is trivial.
  For the other, let $u \in \Span(v_1, \ldots v_m) \cap \cB$.
  Then there exists $c' \in \bbR^k$, $u = \sum_{k=1}^m c'_k v_k$.
  Suppose that there exist $k \neq k'$ such that $c'_k = c'_{k'}$.
  Then since $\lVert x - u \rVert_\infty = \max_k |c_k - c'_k|$ and $|c_k - c_{k'}| > \varepsilon_c$, one has:
  \begin{align*}
     \varepsilon_c < |c_k - c_{k'}|
     &= |c_k - c'_k + c'_{k'} - c_{k'}| \\
     &\leq |c_k - c'_k| + |c'_{k'} - c_{k'}| \\
     &\leq 2 \lVert x - u \rVert_\infty \\
     &\leq  \varepsilon_c \, .
  \end{align*}

  This shows that clusters of any $u \in \cM$ are equal to clusters of $x$.
  In addition, the tangent space of $\cM$ at $x$ is $\Span(v_1, \hdots, v_m)$. 
  
  
  % $\{ u \in \bbR^d : (\exists c' \in \bbR^m, \forall i \neq j \in [d], c'_i \neq c'_j) \& (\forall k \in [m], \forall j \in \cC_k, |u_j| = c'_k) \}$.

  
%   $k(j):[d] \rightarrow [m]$  such that $j \in \cC_{k(j}$.
%  \begin{equation}
%     \cM = \{ u \in \bbR^d : u_j = \sign(x_j) c'_{k(j)}\mbox{ where }  c' \in \bbR^m, \forall i \neq l , c'_i \neq c'_l \} \cap \cB \, .
%   \end{equation}

\begin{enumerate}
  \item The set $\cM$ is then the intersection of a linear subspace and an open ball hence it is a $\cC^2$ manifold. 
  Following the observation of the fact that the clusters of any $u\in\cM$ are the same than the clusters of $x$, we have that 
  \begin{align}{}
    J(u) = \sum_{k=1}^m \left( \sum_{j \in \cC_k} \lambda_j \right)c_k' \enspace ,
  \end{align}
  hence $J$ is linear on $\cM$ thus $\cC^2$. 
  \item The subdifferential of $J$ at $x$ is 
  \begin{align}
    \partial J(x) = 
  \end{align}
  \klopfe{Compute what is the linear hull of subdifferential restricted to one cluster}
  \item The subdifferential of $J$ is a constant set locally around $x$ along $\cM$ which shows that it is continuous at $x$ relative to $\cM$. 
\end{enumerate}

% We will now establish that $J$ is linear on $\mathcal{M}$.
%   let $v_k \in \bbR^d$ for $k \in [m]$  be equal to $\sign x_{\cC_k}$ on $\cC_k$ and to 0 outside, such that $x = \sum_{k=1}^m c_k v_k$.

%   Note that for $u =\sum_{k=1}^m c'_k v_k\in \Span(v_1, \ldots v_m) \cap \cB$ the function $J(u) = \sum_{k=1}^m \left( \sum_{j \in \cC_k} \lambda_j \right)c_k'$ which is clearly linear hence 1 and 3 holds.




  $J$ is linear hence smooth on $\cM$, its gradient is TODO $\sum_k (\sum_{j \in \cC_k} \lambda_j) v_k$. \mm{and this is orthogonal to $\cM$ because??? it cannot be, it's in $\cM$}

  If one cluster of $x$ vanishes ($c_m = 0)$, the same reasoning holds wih the manifold $cM \cap \{u \in \bbR^d: u_{\cC_m} = 0\}$ aka $c'_m = 0$).
\end{proof}

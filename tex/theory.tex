%!TEX root = ./main.tex

\section{Theory}\label{sec:theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Coordinate Descent for SLOPE}%
\label{sec:coordinate-updates}

Proximal coordinate descent cannot be applied to \Cref{pb:slope} because it is not separable.
However, if the clusters $\mathcal{C}_1^*, \ldots, \mathcal{C}_{m^*}^*$ of the solution $\beta^*$ were known together with their signs, then the values $c_1^*, \ldots, c_{\hat{m}}^*$ taken by $\beta^*$ on the clusters could be equivalently computed by solving:
\mm{TODO MM: put in prop and write proof?}
\begin{problem}
\min_{z \in \bbR^{m^*}}
\ell \left(\sum_{i=1}^{m^*} \sum_{j \in \mathcal{C}_i} z_i \sign(\beta_j^*) e_j \right)
%\tilde{x}_{\mathcal{C}_i} z_i \right)
+ \sum_{i=1}^{m^*} | z_i | \sum_{\lambda \in \lambda^{\mathcal{C}_i^*}} \lambda \, ,
\end{problem}
% \klopfe{Clarify what $F$ is here}
% MM: removed F and used the form (ell(beta) instead of F(Xbeta)) : more general
% where $\tilde{x}_{\mathcal{C}_i} = \sum_{j \in \mathcal{C}_i} x_j \sign (\beta_j^*)$.
Conditionally on the knowledge of the clusters and the signs of the coefficients, the penalty becomes separable and coordinate descent could be used.
Hence, the idea of our algorithm is to intertwine steps that identify the clusters, and large coordinate-wise steps on these clusters.

Based on this observation, we derive a coordinate descent algorithm for minimizing the SLOPE problem~\eqref{pb:slope} with respect to the coefficients of a single cluster at a time.

In all the sequel, $\beta$ is fixed, with $m$ clusters $\mathcal{C}_1, \ldots, \mathcal{C}_m$ corresponding to values $c_1, \ldots, c_m$.
Let also $k \in [m]$ be fixed, let $s = \sign \beta_{\mathcal{C}_k}$.
We are interested in updating $\beta$ by changing only the value taken on the $k$-th cluster.
To this end, we let
\begin{equation}
  \label{eq:coordinate-update-beta}
  \beta_i(z) =
  \begin{cases}
    s_k z   \, , & \text{if } i \in \mathcal{C}_k \, , \\
    \beta_i \, , & \text{otherwise} \, .
  \end{cases}
\end{equation}
% This means that \(\beta(z)\) corresponds to a version of \(\beta\) with a
% coordinate update for the \(k\)-th cluster.
Minimizing the objective in this directions amounts to solving the following
one-dimensional problem:

\begin{problem}
\label{pb:cluster-problem}
\min_{z \in \mathbb{R}} \Big(
G(z) = P(\beta(z))  = F(\beta(z)) + \phi(z)
\Big) \,  ,
\end{problem}
where
\[
  \phi(z) = |z| \sum_{j \in \mathcal{C}_k} \lambda_{(j)^-_z}
  + \sum_{j \notin \mathcal{C}_k} |\beta_j| \lambda_{(j)^-_z}
\]
is the \emph{partial sorted \(\ell_1\) norm} with respect to the \(k\)-th cluster and where we write \(\lambda_{(j)^-_z}\) to indicate that the inverse sorting permutation \((j)^-_z\)
is defined with respect to \(\beta(z)\).
The optimality condition for \Cref{pb:cluster-problem} is
\[
  % P_k'(z; \delta) \geq 0,
  G'(z; \delta) \geq 0,
\]
where $G'(z; \delta) $ is the directional derivative in the direction $\delta$.
Since \(F\) is differentiable we have
\[
  G'(z; \delta)  = \delta \nabla F(\beta(z)) + \phi'(z; \delta) \, ,
\]
% since \(\lVert \tilde{r} - \tilde{x}z\rVert_2^2\) is differentiable.
where \(\phi'(z; \delta)\) is the directional derivative of $\phi$.



Throughout the rest of this section we derive a \mm{the?} solution to \eqref{pb:cluster-problem}.
To do so, we will introduce the directional derivative for the
sorted \(\ell_1\) norm with respect to the coefficient of the \(k\)-th cluster.
First, let \(C(z)\) be a function that returns the cluster corresponding to \(z\), that is
\[
  C(z) = \{j : |\beta(z)_j| = z\}.
\]
Next, let $\varepsilon_c$ be an arbitrary positive value such that
\begin{equation}
  \label{eq:epsilon-c}
  \varepsilon_c < \big| c_i - c_j\big| , \quad \forall\, i \neq j \text{ and } \varepsilon_c < c_m \text{ if } c_m \neq 0 \, .
\end{equation}
% For a \({\varepsilon_c}\) and
Then for \(\delta \in \{-1, 1\}\) we have
\begin{equation}
  \begin{aligned}
    \lim_{h \downarrow 0} C(z + h\delta)                & = C(z + {\varepsilon_c} \delta),               \\
    \lim_{h \downarrow 0} \lambda_{(i)^-_{z + h\delta}} & = \lambda_{(i)^-_{z + {\varepsilon_c}\delta}}.
  \end{aligned}
\end{equation}
In other words, the order permutation corresponding to \(\beta(z + h\delta)\)
depends only on \(\delta\) as \(h\) tends to~\(0\).
\begin{remark}
  As consequence of the definition of \(\varepsilon_c\), the order permutations
  for \(\beta(z)\) and \(\beta(z + {\varepsilon_c} \delta)\) differ only for a
  subset of the permutation vectors.
  The permutation for \(\beta(h\delta)\)
  corresponds to
  \begin{equation*}
    \begin{cases}
      \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, \mathcal{C}_m, C({\varepsilon_c}\delta)
       & \text{if } c_m > 0 \, , \\ % \text{ and } \delta \neq 0 \, , \\
      \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, C({\varepsilon_c}\delta), \mathcal{C}_m
       & \text{if } c_m = 0 \, . %\text{ and } \delta \neq 0 \, . \\
      % \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, \underbrace{\mathcal{C}_k \cup \mathcal{C}_m}_{C({\varepsilon_c}\delta)} & \text{if } c_m = 0 \text{ and } \delta = 0.    \\
    \end{cases}
  \end{equation*}
  If \(z \neq 0\), the permutation for \(\beta(z + {\varepsilon_c} \delta)\) corresponds to
  \[
    \begin{cases}
      \splitfrac{\mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, }{C(c_i + {\varepsilon_c}\delta), \mathcal{C}_k, \dots, \mathcal{C}_m} & \text{if } \delta = 1 \,,   \\
      \splitfrac{\mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots,}{ \mathcal{C}_k, C(c_k + {\varepsilon_c}\delta), \dots, \mathcal{C}_m} & \text{if } \delta = -1 \, . \\
      % \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, \underbrace{\mathcal{C}_i \cup \mathcal{C}_k}_{C(c_i + {\varepsilon_c}\delta)}, \dots, \mathcal{C}_m & \text{if } \delta = 0.  \\
    \end{cases}
  \]
\end{remark}
%, defined as
% \(c^{(k)} = \{c_1, c_2, \dots, c_m\} \setminus c_k\), such that
% \[
%   c_i^{(k)} =
%   \begin{cases}
%     c_i     & \text{if } i < k,    \\
%     c_{i+1} & \text{if } i \geq k.
%   \end{cases}
% \]

We are now ready to state the directional derivative of $\phi$. % \(J_k\).

\begin{theorem}\label{thm:sl1-directional-derivative}
  Let \(c^{\setminus k}\) be the \(m - 1\) length version of $c$ where the $k$-th coordinate has been removed: $c^{\setminus k} = (c_1, \ldots c_{k-1}, c_{k+1}, \ldots, c_m)$.
  Let \({\varepsilon_c}\) defined as in \eqref{eq:epsilon-c}.
  The directional derivative of the partial sorted $\ell_1$ norm with respect to the $k$-th cluster \(\phi\), in the direction \(\delta\) is
  \[
    \phi'(z; \delta) =
    \begin{cases}
      \smashoperator[r]{\sum_{j \in C(\varepsilon_c )}} \lambda_{(j)^-_{\varepsilon_c }}
       & \text{if } z = 0 \, ,               \\
      \sign(z)\delta\smashoperator{\sum_{j \in C(z + {\varepsilon_c} \delta)}} \lambda_{(j)^-_{z + {\varepsilon_c}\delta}}
       & \text{if } |z| = c^{(k)}_i > 0 \, , \\
      \sign(z)\delta\smashoperator{\sum_{j \in C(z)}} \lambda_{(j)^-_{z}}
       & \text{otherwise} \, .
    \end{cases}
  \]
\end{theorem}

In \cref{fig:directional-derivative}, we show an example of the directional
derivative and the objective function.

\begin{figure}[htb]
  \centering
  \includegraphics[]{directional-derivative.pdf}
  \caption{%
  The function \(P_k(z)\) and its directional derivative \(P'_k(z; \delta)\) for
  an example with \(\beta = [-3.0, 1.0, 3.0, 2.0]^T\), \(k = 1\), and consequently
  \(c^{\setminus k} = [2, 1]^T\). The solution corresponds to the value of \(z\) for
      which \(P'_k(z; \delta) \geq 0 \) for \(\delta \in \{-1, 1\}\), which holds only
      at \(z = 2\), which must therefore be the solution.
    }
  \label{fig:directional-derivative}
\end{figure}

We are now ready to present the SLOPE thresholding operator.

\begin{theorem}[The SLOPE Thresholding Operator]
  \label{thm:thresholding-operator}
  Define \(S(x) = \sum_{j \in C(x)}\lambda_{(j)^-_{x}}\) and
  let
  \[
    \begin{multlined}
      T_k(\gamma; \omega, c, \lambda) = \\
      % \begin{cases}
      %   0                                                                                                                                                   & \text{if } |\gamma| \leq \smashoperator{\sum_{j \in C({\varepsilon_c})}} \lambda_{(j)^-_{\varepsilon_c}},                                                                                                                                                        \\
      %   \sign(\gamma)c_i                                                                                                                                    & \text{if } \omega c_i + \smashoperator{\sum_{j \in C(c_i - {\varepsilon_c})}} \lambda_{(j)^-_{c_i - {\varepsilon_c}}} \leq |\gamma| \leq \omega c_i + \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}},             \\
      %   \frac{\sign(\gamma)}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}} \bigg) & \text{if } \omega c_i + \smashoperator{\sum_{j \in C(c_i + {\varepsilon_c})}} \lambda_{(j)^-_{c_i + {\varepsilon_c}}} < |\gamma| < \omega c_{i - 1} + \smashoperator{\sum_{j \in C(c_{i - 1} - {\varepsilon_c})}} \lambda_{(j)^-_{c_{i - 1} - {\varepsilon_c}}}, \\
      %   \frac{\sign(\gamma)}{\omega} \bigg( |\gamma| - \smashoperator{\sum_{j \in C(c_1 + {\varepsilon_c})}} \lambda_{(j)^-_{c_1 + {\varepsilon_c}}} \bigg) & \text{if } |\gamma| \geq \omega c_1 + \smashoperator{\sum_{j \in C(c_1 + {\varepsilon_c})}} \lambda_{(j)^-_{c_1 + {\varepsilon_c}}}.
      % \end{cases}
      \begin{cases}
        0                                                                            & \text{if } |\gamma| \leq S(\varepsilon_c),               \\
        \sign(\gamma)c_i                                                             & \text{if } \omega c_i + S(c_i - \varepsilon_c)           \\
                                                                                     & \quad \leq |\gamma| \leq                                 \\
                                                                                     & \quad \omega c_i + S(c_i + \varepsilon_c),               \\
        \frac{\sign(\gamma)}{\omega} \big( |\gamma| - S(c_i + \varepsilon_c) \big)   & \text{if } \omega c_i + S(c_i + {\varepsilon_c})         \\
                                                                                     & \quad < |\gamma| <                                       \\
                                                                                     & \quad \omega c_{i - 1} + S(c_{i - 1} - {\varepsilon_c}), \\
        \frac{\sign(\gamma)}{\omega} \big( |\gamma| - S(c_1 + {\varepsilon_c}) \big) & \text{if } |\gamma| \geq                                 \\
                                                                                     & \quad \omega c_1 + S(c_1 + {\varepsilon_c}).
      \end{cases}
    \end{multlined}
  \]
  with \({\varepsilon_c}\) defined as in \eqref{eq:epsilon-c} and let
  \(\gamma = \tilde{r}^Tx\), \(\omega = \tilde{x}^T\tilde{x}\). Then
  \(T(\gamma; \omega, c^{(k)}, \lambda) \in \argmin_{z \in \mathbb{R}} P_k(z)\).
\end{theorem}

\jl{Consider adding a remark showing that our operator generalizes the soft thresholding operator.}

In \cref{fig:slope-thresholding}, we visualize the SLOPE thresholding operator.

\begin{figure*}[htb]
  \centering
  \includegraphics[]{slope-thresholding.pdf}
  \caption{%
  An example of the SLOPE thresholding operator. The result corresponds to an
  example for \(\beta = [0.5, -0.5, 0.3, 0.7]^T\), \(c = [0.7, 0.5, 0.3]^T\)
  with an update for the second cluster (\(k = 2\)), such that
  \(c^{\setminus k} = [0.5, 0.3]^T\). Across regions where the function is constant,
      the operator sets the result to be either exactly 0 or to the value of one
      of the elements of \(c^{\setminus k}\).
    }
  \label{fig:slope-thresholding}
\end{figure*}

\subsubsection{Naive Updates}

As in \textcite{friedman2010}, we can improve the efficiency of updates by observing that
\begin{equation*}
  \begin{aligned}
    \tilde r_k & = y - \tilde y_k                                                                       \\
               & = y - X_{\bar{\mathcal{C}}_k}\beta_{\bar{\mathcal{C}}_k} - \tilde x c_k + \tilde x c_k \\
               & = r + \tilde x c_k
  \end{aligned}
\end{equation*}
and therefore that
\begin{equation}
  \label{eq:naive-update}
  \tilde x_k^T (y - \tilde y_k) = \tilde x_k^T r + \tilde x_k^T \tilde x_k c_k.
\end{equation}

\subsubsection{Caching Reductions}

Observe that \(\tilde x_k\) only changes between subsequent coordinate updates provided that the members of the cluster \(k\) change, for instance if two clusters are merged, a predictor leaves a cluster, or the signs flip (through an update of \(\alpha_k\)).
As a result, it is possible to obtain computational gains by caching \(\tilde x_k\) and \(\tilde x_k^T \tilde x_k\) for each cluster (except the zero cluster, which we do not consider in our coordinate descent step).
When there is no change in the clusters, there is no need to recompute these quantities.
And even when there are changes, we can still reduce the costs involved since \(\tilde x_k\) can be updated in place.
If a large cluster is joined by few new predictors, then the cost of updating may be much lower than recomputing the quantities for the entire cluster.
Also note that, for single-member clusters we only need to store \(\tilde x_k^T \tilde x_k\) since \(\tilde x_k\) is just a column in \(X\) times the corresponding sign.

Letting \(\tilde x_k^\text{old}\) correspond to the value of \(\tilde x_k\) before the update, we note that \(\tilde x_k \gets \tilde x_k^\text{old} + x_j \sign(\beta_j)\) for each \(j \in \mathcal{C}_k^\text{new} \setminus \mathcal{C}_k^\text{old}\) and \(\tilde x_k \gets \tilde x_k^\text{old} - x_j \sign(\beta_j)\) for each \(j \in \mathcal{C}_k^\text{old} \setminus \mathcal{C}_k^\text{new}\).
If only the signs flip, we simply have to also flip the signs in \(\tilde x_k\).

\subsubsection{Covariance Updates}

Notice that we can rewrite the first term in \eqref{eq:naive-update} as
\begin{equation}
  \begin{aligned}
    \tilde x_k^T r & = \tilde x_k^T y - \sum_{j : \beta_j \neq 0} \tilde x_k^T x_j \beta_j                                                                    \\
                   & = \tilde x_k^T y - \sum_{j : c_j \neq 0} \tilde x_k^T \tilde x_j c_j                                                                     \\
                   & = s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T y - \sum_{j : \beta_j \neq 0} s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T x_j \beta_j              \\
                   & = s_{\mathcal{C}_k}^T \left(X^T y\right)_{\mathcal{C}_k} - \sum_{j : \beta_j \neq 0} s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T x_j \beta_j \\
                   & = \sum_{j \in \mathcal{C}_k}\left( s_j x_j^Ty - \sum_{t : \beta_t \neq 0} s_j x_j^T x_t \beta_t \right)
  \end{aligned}
\end{equation}
As in \textcite{friedman2010}, this formulation can be used to achieve so-called \emph{covariance updates}.
We compute \(X^T y\) once at the start.
Then, each time a new predictor becomes non-zero, we compute its inner product with all other predictors, caching these products.

\subsection{Hybrid proximal coordinate descent strategy}

We propose an iterative solver that alternates between proximal gradient step and proximal coordinate descent.
Since the regularization term for SLOPE is not separable, applying PCD does not guarantee convergence.
However, \cite{dupuis2021} showed that once the clusters are known, the subdifferential of $J$ can be written as the cartesian product of the subdifferential of $J$ restricted to the clusters.
Hence, if one knew the clusters, PCD updates could be applied on each cluster.

The notion of clusters for SLOPE extends the notion of sparsity coming from the LASSO.
Identification of the sparsity pattern throughout the iterative algorithm have largely been studied.
Talk about Partly Smooth functions, related Manifold and that support transpose to cluster for SLOPE regularization. DO the maths.

Then identification of this underlying structure occurs when applying PGD.
Hence the idea to alternate, PGD and PCD step to take advantage of the speed of PCD and ensure convergence via the identification of the right structure with PGD steps.

\begin{algorithm}[tb]
  \SetKwInOut{Init}{init}
  \SetKwInOut{Input}{input}
  \caption{%
    Hybrid coordinate descent and proximal gradient descent algorithm
    for SLOPE\label{alg:hybrid}}
  \Input{
    \(X \in \mathbb{R}^{n\times p}, y\in \mathbb{R}^n, \beta\in \mathbb{R}^p, \lambda \in \{\mathbb{R}^p : \lambda_1 \geq \lambda_2 \geq \cdots > 0\}\), \(m \in \mathbb{N}\)}

  \Init{\(t \gets 0\), \(\beta \gets 0\), \(L \gets \lVert X \rVert_2^2\)}

  \Repeat{convergence}{
  \(t \gets t + 1\)

  \If{\(t \bmod m = 0\)}{

  \(\beta \leftarrow \operatorname{prox}_{J / L}(\beta - \frac{1}{L}\nabla f(\beta))\) \label{alg:hybrid-istastep}

  % \(C_1, \hdots, C_m \leftarrow \mathtt{get\_clusters}(\beta)\)
  }
  \Else{
    \(k \gets 0\)

    \While{\(k \leq \lvert \mathcal{C} \rvert\)}{
      \(k \gets k + 1\)

      \(s \leftarrow \mathrm{sign}(\beta_{\mathcal{C}_k})\)

      \If{\(s \neq 0\)}{
        % \(L_k \gets (X_{:, \mathcal{C}_k}s)^\top  X_{:, \mathcal{C}_k}s\)


        % \(\tilde{\beta} \gets T(|\beta_{\mathcal{C}_k}| - \frac{1}{L_k} \nabla_{\mathcal{C}_k}f(\beta)^\top s, \frac{\lambda}{L_k})\)

        % \(\beta_{\mathcal{C}_k} \leftarrow \tilde{\beta} s\)
        \(\tilde x_k \gets X_{\mathcal{C}_k}s\)

        \(\tilde y_k \gets X_{\widebar{\mathcal{C}}_k}\beta_{\widebar{\mathcal{C}}_k}\)

        \(\tilde r_k \gets y - \tilde y_k\)

        \(
        \beta_{\mathcal{C}_k} \gets
        s T \left(
        \frac{\tilde r^T_k \tilde x_k, }{ \tilde x_k^T \tilde x_k},
        \frac{\lambda}{ \tilde x_k^T \tilde x_k},
        k,
        \tilde \beta,
        \mathcal{C}
        \right)
        \)
        \Comment{\(\mathcal{C}\) is updated at this step.}


      }

    }
  }

  }
  \Return{\(\beta\)}
\end{algorithm}

In \cref{fig:illustration-solver}, we show how \cref{alg:hybrid} works in practice on
a two-dimensional SLOPE problem.

\begin{figure*}[htbp]
  \centering
  \includegraphics{illustration_solvers}
  \caption{Illustration of the proposed solver. The figures show progress
    until convergence for the coordinate descent (CD) solver that we use as part
    of the hybrid method, our hybrid method, and  proximal gradient descent
    (PGD). The orange cross marks the optimum. Dotted lines indicate where the
    coefficients are equal in absolute value. The dashed lines indicate PGD
    steps and solid lines CD steps. Each dot marks a complete epoch, which may
    correspond to only a single coefficient update for the CD and hybrid
    solvers if the coefficients flip order. Each solver was run until the duality
    gap was smaller than \(10^{-10}\). Note that the CD algorithm cannot split clusters
    and is therefore stuck after the third epoch. The hybrid and PGD algorithms,
    meanwhile, reach convergence after 67 and 156 epochs respectively.}
  \label{fig:illustration-solver}
\end{figure*}

\begin{lemma}
  \jw{Use the same notation as in the Algorithm.}
  Let \(\beta^{(1)}, \beta^{(2)}, \dots, \beta^{(k)}\) be a sequence of
  iterates generated by \cref{alg:hybrid}, \(1/t\) the frequency of proximal gradient
  descent iterates in \cref{alg:hybrid}, and \(L\) the Lipschitz constant of
  \(\nabla \ell\). Then
  \[
    P(\beta^{(k)}) - P(\beta^*) \leq \frac{L \lVert \beta^{(0)} - \beta^* \rVert_2^2}{2\lfloor k/t \rfloor }\,.
  \]
  Here \(  \beta^* \in \argmin_{\beta \in \mathbb{R}^p} P.\)
\end{lemma}
\begin{proof}
  Assume that we are at iteration \(k + 1\) \jw{We only have k iterations?} of \cref{alg:hybrid} and observe
  that \cref{pb:cluster-problem} can be written as a constrained form of \cref{pb:slope}:
  \[
    \begin{aligned}
       & \min_{\beta \in \mathbb{R}^p} &  & P(\beta),                                                                                                     \\
       & \operatorname{subject\;to}    &  & |\beta_i| = |\beta_j|                                                   &  & i,j \in \mathcal{C}_k, i \neq j, \\
       &                               &  & \sign(\beta_i)\sign(\beta_i^{(k)}) = \sign(\beta_j)\sign(\beta_j^{(k)}) &  & i,j \in \mathcal{C}_k,           \\
       &                               &  & \beta_i = \beta_i^{(k)}                                                 &  & i \notin \mathcal{C}_k.
    \end{aligned}
  \]
  \jw{I don't think we need this level of detail. Just use that line 9-14 corresponds to $\beta^{t+k/|C|}= T_k(\beta^{t+(k-1)/|C|})$ and by theroem we have that $P(\beta^{t+k/|C|}) \leq P(beta^{t+(k-1)/|C|})$ approxiamtly}
  And note that these constraints hold for any \(\beta^{(k)}\) used as input to this
  problem.
  In \cref{thm:thresholding-operator}, we showed that the thresholding operator
  \(T\) minimizes this problem for any \(\beta^{(k)}\), which means that
  \(P(\beta^{(k)}) \leq P(\beta^{(k + 1)})\) for any coordinate descent iteration
  \(k + 1\) in \cref{alg:hybrid}.

  The convergence properties of proximal gradient descent are well
  known~\parencite{beck2009,daubechies2004}, and hence we know that for a
  sequence of iterates of the proximal gradient step~(\cref{alg:hybrid-istastep}),
  \jw{I suggest making it more formal. Cite one theorem in one article where it comes from. }
  \(\beta^{(t)}, \beta^{(2t)}, \dots, \beta^{(\lfloor k / t \rfloor)}\), it holds that
  \[
    P(\beta^{(\lfloor k / t \rfloor)}) - P(\beta^*)
    \leq \frac{L \lVert \beta^{(0)} - \beta^* \rVert_2^2}{2\lfloor k / t \rfloor}.
  \]
  Combining this and the result from the previous paragraph yields the desired
  result.
\end{proof}


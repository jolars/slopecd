%!TEX root = ./main.tex
\section{INTRODUCTION}\label{sec:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper we present a novel numerical algorithm for Sorted L-One Penalized
Estimation (SLOPE, ~\cite{bogdan2013,bogdan2015,zeng2014ordered}), defined as
\begin{problem}\label{pb:slope}
  \min_{\beta \in \mathbb{R}^p}
  P(\beta) =  \frac{1}{2 n} \norm{y - X \beta}^2 + J(\beta)
\end{problem}
% where we take \(F\) to be smooth and twice differentiable and
where
\begin{equation}
  \label{eq:sorted-l1-norm}
  J(\beta) = \sum_{j=1}^p \lambda_j|\beta_{(j)}|
\end{equation}
is the \emph{sorted \(\ell_1\) norm}, defined through
\begin{equation*}
  |\beta_{(1)}| \geq |\beta_{(2)}| \geq \cdots \geq |\beta_{(p)}| \enspace,
\end{equation*}
with \(\lambda\) being a fixed non-increasing and non-negative sequence.

The sorted $\ell_1$ norm is a sparsity-enforcing penalty that has become
increasingly popular due to several appealing properties, such as its ability
to control false discovery rate~\parencite{bogdan2015,kos2020}, cluster
coefficients~\parencite{figueiredo2016, schneider2020a}, and recover sparsity and
ordering patterns in the solution~\parencite{bogdan2022}. Unlike other competing
sparse regularization methods such as MCP~\parencite{zhang2010} and
SCAD~\parencite{fan2001}, SLOPE has the advantage of being a convex problem~\parencite{bogdan2015}.

In spite of the availability of predictor screening
rules~\parencite{larsson2020c,elvira2022}, which help speed up SLOPE in the
high-dimensional regime, current state-of-the-art algorithms for SLOPE perform
poorly in comparison to those of more established penalization methods such as
the lasso (\(\ell_1\)-norm regularization) and ridge regression
(\(\ell_2\)-norm regularization).
As a small illustration of this issue, we compared the speed at which the \pkg{SLOPE}~\parencite{friedman2022} and \pkg{glmnet}~\parencite{friedman2022} packages solve a SLOPE and lasso problem, respectively, for the \dataset{bcTCGA} data set.
In this small experiment, \pkg{SLOPE} takes 43 seconds to reach convergence, whilst \pkg{glmnet} requires only 0.14 seconds\footnote{See~\Cref{sec:slope-vs-glmnet} for details on how this experiment was run.}.

This lackluster performance has hampered the applicability of SLOPE to many
real-world applications. A major reason for why algorithms for solving
$\ell_1$-, MCP-, or SCAD-regularized problems enjoy better performance is that
they use coordinate
descent~\parencite{tseng2001convergence,friedman2010,breheny2011}. Current SLOPE
solvers, on the other hand, rely on proximal gradient descent algorithms such
as FISTA~\parencite{beck2009} and the alternating direction method of multipliers
method (ADMM, \cite{boyd2010}), which have proven to be less efficient than
coordinate descent in empirical benchmarks on related problems, such as the
lasso~\parencite{moreau2022benchopt}. Applying coordinate descent to SLOPE is not,
however, straightforward since convergence guarantees for coordinate descent
require the objective to be separable, which is not the case for SLOPE. As a
result, naive coordinate descent schemes can get
stuck~(\Cref{fig:naive-cd-stuck}).

\begin{figure}[htb]
  \centering
  \includegraphics[]{naive-cd-stuck.pdf}
  \caption{%
    An example of (standard) coordinate descent getting stuck on a two-dimensional SLOPE problem.
    The main plot shows level curves for the primal objective~\eqref{pb:slope}, with the minimizer \(\beta^* = [0, 0]^T\) indicated by the orange cross.
    The marginal plots displays objective values at \(\beta_1 = 0.2\) when optimizing over \(\beta_2\) and vice versa.
    At \(\beta = [0.2,0.2]^T\), standard coordinate descent can only move in the directions indicated by the dashed lines---neither of which are descent directions for the objective.
    As a result, the algorithm is stuck at a suboptimal point.
  }
  \label{fig:naive-cd-stuck}
\end{figure}

\jl{We should probably mention the Newt-ALM solver somewhere in the introduction, but I'm not quite sure what the right angle is.}

In this article we address this problem by introducing a new, highly effective
algorithm for SLOPE based on a hybrid proximal gradient and coordinate descent
scheme. Our method features convergence guarantees and reduces the time
required to fit SLOPE by orders of magnitude in our empirical experiments.

@article{beck2009,
  title = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
  author = {Beck, A. and Teboulle, M.},
  date = {2009-01-01},
  journaltitle = {SIAM Journal on Imaging Sciences},
  shortjournal = {SIAM J. Imaging Sci.},
  volume = {2},
  number = {1},
  pages = {183--202},
  doi = {10.1137/080716542},
  url = {https://epubs.siam.org/doi/abs/10.1137/080716542},
  urldate = {2019-02-10},
  abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.}
}

@unpublished{bogdan2013,
  title = {Statistical Estimation and Testing via the Sorted {{L1}} Norm},
  author = {Bogdan, Ma≈Çgorzata and family=Berg, given=Ewout, prefix=van den, useprefix=false and Su, Weijie and Cand√®s, Emmanuel},
  date = {2013-10-29},
  eprint = {1310.1969},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1310.1969},
  urldate = {2020-04-16},
  abstract = {We introduce a novel method for sparse regression and variable selection, which is inspired by modern ideas in multiple testing. Imagine we have observations from the linear model y = X beta + z, then we suggest estimating the regression coefficients by means of a new estimator called SLOPE, which is the solution to minimize 0.5 ||y - Xb\textbackslash |\_2\^2 + lambda\_1 |b|\_(1) + lambda\_2 |b|\_(2) + ... + lambda\_p |b|\_(p); here, lambda\_1 {$>$}= \textbackslash lambda\_2 {$>$}= ... {$>$}= \textbackslash lambda\_p {$>$}= 0 and |b|\_(1) {$>$}= |b|\_(2) {$>$}= ... {$>$}= |b|\_(p) is the order statistic of the magnitudes of b. The regularizer is a sorted L1 norm which penalizes the regression coefficients according to their rank: the higher the rank, the larger the penalty. This is similar to the famous BHq procedure [Benjamini and Hochberg, 1995], which compares the value of a test statistic taken from a family to a critical threshold that depends on its rank in the family. SLOPE is a convex program and we demonstrate an efficient algorithm for computing the solution. We prove that for orthogonal designs with p variables, taking lambda\_i = F\^\{-1\}(1-q\_i) (F is the cdf of the errors), q\_i = iq/(2p), controls the false discovery rate (FDR) for variable selection. When the design matrix is nonorthogonal there are inherent limitations on the FDR level and the power which can be obtained with model selection methods based on L1-like penalties. However, whenever the columns of the design matrix are not strongly correlated, we demonstrate empirically that it is possible to select the parameters lambda\_i as to obtain FDR control at a reasonable level as long as the number of nonzero coefficients is not too large. At the same time, the procedure exhibits increased power over the lasso, which treats all coefficients equally. The paper illustrates further estimation properties of the new selection rule through comprehensive simulation studies.},
  archiveprefix = {arXiv}
}

@article{bogdan2015,
  title = {{{SLOPE}} ‚Äì Adaptive Variable Selection via Convex Optimization},
  author = {Bogdan, Ma≈Çgorzata and family=Berg, given=Ewout, prefix=van den, useprefix=false and {Chiara Sabatti} and {Weijie Su} and {Emmanuel J. Cand√®s}},
  date = {2015-09},
  journaltitle = {The annals of applied statistics},
  shortjournal = {Ann Appl Stat},
  volume = {9},
  number = {3},
  eprint = {26709357},
  eprinttype = {pmid},
  pages = {1103--1140},
  issn = {1932-6157},
  doi = {10.1214/15-AOAS842},
  url = {https://projecteuclid.org/euclid.aoas/1446488733},
  urldate = {2018-12-17}
}

@article{daubechies2004,
  title = {An Iterative Thresholding Algorithm for Linear Inverse Problems with a Sparsity Constraint},
  author = {Daubechies, I. and Defrise, M. and De Mol, C.},
  date = {2004-08-26},
  journaltitle = {Communications on Pure and Applied Mathematics},
  volume = {57},
  number = {11},
  pages = {1413--1457},
  issn = {1097-0312},
  doi = {10.1002/cpa.20042},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20042},
  urldate = {2022-05-27},
  abstract = {We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted ùìÅp-penalties on the coefficients of such expansions, with 1 ‚â§ p ‚â§ 2, still regularizes the problem. Use of such ùìÅp-penalized problems with p {$<$} 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. ¬© 2004 Wiley Periodicals, Inc.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.20042}
}

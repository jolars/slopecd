%!TEX root = ./main.tex

\section{Theory}\label{sec:theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Coordinate Descent for SLOPE}%
\label{sec:coordinate-updates}

Proximal coordinate descent cannot be applied to \Cref{pb:slope} because it is not separable.
However, if the clusters $\mathcal{C}_1^*, \ldots, \mathcal{C}_{m^*}^*$ of the solution $\beta^*$ were known together with their signs, then the values $c_1^*, \ldots, c_{m^*}^*$ taken by $\beta^*$ on the clusters could be equivalently computed by solving:
\begin{problem}
\min_{z \in \bbR^{m^*}}
\ell \left(\sum_{i=1}^{m^*} \sum_{j \in \mathcal{C}_i^*} z_i \sign(\beta_j^*) e_j \right)
+ \sum_{i=1}^{m^*} | z_i | \sum_{j \in \mathcal{C}_i^*} \lambda_j \, ,
\end{problem}
% \klopfe{Clarify what $F$ is here}
% MM: removed F and used the form (ell(beta) instead of F(Xbeta)) : more general
% where $\tilde{x}_{\mathcal{C}_i} = \sum_{j \in \mathcal{C}_i} x_j \sign (\beta_j^*)$.
Conditionally on the knowledge of the clusters and the signs of the coefficients, the penalty becomes separable and coordinate descent could be used.
Hence, the idea of our algorithm is to intertwine steps that identify the clusters, and large coordinate-wise steps on these clusters.

Based on this observation, we derive a coordinate descent algorithm for minimizing the SLOPE problem~\eqref{pb:slope} with respect to the coefficients of a single cluster at a time.

In all the sequel, $\beta$ is fixed, with $m$ clusters $\mathcal{C}_1, \ldots, \mathcal{C}_m$ corresponding to values $c_1, \ldots, c_m$.
Let also $k \in [m]$ be fixed, let $s = \sign \beta_{\mathcal{C}_k}$.
We are interested in updating $\beta$ by changing only the value taken on the $k$-th cluster.
To this end, we let
\begin{equation}
  \label{eq:coordinate-update-beta}
  \beta_i(z) =
  \begin{cases}
    s_k z   \, , & \text{if } i \in \mathcal{C}_k \, , \\
    \beta_i \, , & \text{otherwise} \, .
  \end{cases}
\end{equation}
% This means that \(\beta(z)\) corresponds to a version of \(\beta\) with a
% coordinate update for the \(k\)-th cluster.
Minimizing the objective in this direction amounts to solving the following
one-dimensional problem:

\begin{problem}
\label{pb:cluster-problem}
\min_{z \in \mathbb{R}} \Big(
G(z) = P(\beta(z))  = F(\beta(z)) + \phi(z)
\Big) \,  ,
\end{problem}
where
\[
  \phi(z) = |z| \sum_{j \in \mathcal{C}_k} \lambda_{(j)^-_z}
  + \sum_{j \notin \mathcal{C}_k} |\beta_j| \lambda_{(j)^-_z}
\]
is the \emph{partial sorted \(\ell_1\) norm} with respect to the \(k\)-th cluster and where we write \(\lambda_{(j)^-_z}\) to indicate that the inverse sorting permutation \((j)^-_z\)
is defined with respect to \(\beta(z)\).
The optimality condition for \Cref{pb:cluster-problem} is
\[
  % P_k'(z; \delta) \geq 0,
  G'(z; \delta) \geq 0,
\]
\mm{above, add: $\forall \delta$ or $\forall \delta \in \{-1, 1\}$ ?}
where $G'(z; \delta) $ is the directional derivative of $G$ in the direction $\delta$.
Since \(F\) is differentiable we have
\[
  G'(z; \delta)  = \delta \sum_{i \in \mathcal{C}_k}\nabla_i F(\beta(z)) + \phi'(z; \delta) \, ,
\]
% since \(\lVert \tilde{r} - \tilde{x}z\rVert_2^2\) is differentiable.
where \(\phi'(z; \delta)\) is the directional derivative of $\phi$.

Throughout the rest of this section we derive the solution to \eqref{pb:cluster-problem}.
To do so, we will introduce the directional derivative for the
sorted \(\ell_1\) norm with respect to the coefficient of the \(k\)-th cluster.
First, as illustrated on \Cref{fig:partial_slope}, note that $\phi$ is piecewise affine, with breakpoints at 0 and all $\pm c_i$'s for $i \neq k$.
Hence, the partial derivative is piecewise constant, with jumps at these points; in addition, $\phi'(\cdot; 1) = \phi'(\cdot, -1)$ except at these points.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{partial_slope.pdf}
  \caption{Graph of the partial sorted $\ell_1$ norm with \(\beta = [-3.0, 1.0, 3.0, 2.0]^T\), \(k = 1\) and so $c_1, c_2, c_3 = (3, 2, 1)$.}
  \label{fig:partial_slope}
\end{figure}

Let \(C(z)\) be the function that returns the cluster of $\beta(z)$ corresponding to \(z\), that is
\[
  C(z) = \{j : |\beta(z)_j| = z\} \,.
\]
Note that, if $z$ is equal to some $c_i$ then $C(z) = \mathcal{C}_i \cup \mathcal{C}_k$ and otherwise $C(z) = \mathcal{C}_k$.

\begin{remark}
  Related to the piecewise affineness of $\phi$ is the fact that the permutation corresponding to $\beta(z)$ is:
  \begin{equation*}
    \begin{cases}
    \cC_k, \cC_m, \ldots, C_1 \, ,
        &\text{ if } z \in \left]0, c_m\right[ \, \\
    \cC_m, \ldots ,\cC_i, \cC_k, \cC_{i-1}, \ldots, C_1 \, ,
        &\text{ if } z \in \left]c_{i}, c_{i-1} \right[, \, i \in \llbracket 2 , m \rrbracket \, \\
    \cC_m, \ldots C_1,  \cC_k, \, ,
        &\text{ if } z \in \left]c_1, +\infty \right[ \, , \\
    \end{cases}
  \end{equation*}
  and that this permutation also reorders $\beta(z \pm h)$ for $z \neq c_i \; (i \neq k)$ and $h$ small enough.
  The only change in permutation happens when $z = 0$ or $z = c_i \; (i \neq k)$.
  Finally, the permutations differ between $\beta(z + h)$ and $\beta(z - h)$ for arbitrarily small $h$ if and only if $z = c_i$ and $z \neq 0$.
\end{remark}

% Then for \(\delta \in \{-1, 1\}\) we have
% \begin{equation}
%   \begin{aligned}
%     \lim_{h \downarrow 0} C(z + h\delta)
%           & = C(z + {\varepsilon_c} \delta) \, ,  \\
%     \lim_{h \downarrow 0} \lambda_{(i)^-_{z + h\delta}}
%           & = \lambda_{(i)^-_{z + {\varepsilon_c}\delta}} \, .
%   \end{aligned}
% \end{equation}
% \mm{nitpick: permutations are not unique because of potential ties, so the second limit may not be true if ties are not handled consistently. We can say when we introduce the sorting that we break ties by picking the lowest index}
% In other words, the order permutation corresponding to \(\beta(z + h\delta)\)
% depends only on \(\delta\) as \(h\) tends to~\(0\).
% \begin{remark}
%   As consequence of the definition of \(\varepsilon_c\), the order permutations
%   for \(\beta(z)\) and \(\beta(z + {\varepsilon_c} \delta)\) differ only for a
%   subset of the permutation vectors.
%   The permutation for \(\beta(h\delta)\) is %\mm{for $h \neq 0$, but if equal to 0, two clusters merge}
%   \begin{equation*}
%     \begin{cases}
%       \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, \mathcal{C}_m, C(\varepsilon_c)
%        & \text{if } c_m > 0 \, , \\ % \text{ and } \delta \neq 0 \, , \\
%       \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, C(\varepsilon_c), \mathcal{C}_m
%        & \text{if } c_m = 0 \, . %\text{ and } \delta \neq 0 \, . \\
%       % \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, \underbrace{\mathcal{C}_k \cup \mathcal{C}_m}_{C({\varepsilon_c}\delta)} & \text{if } c_m = 0 \text{ and } \delta = 0.    \\
%     \end{cases}
%   \end{equation*}
%   If \(z \neq 0\), the permutation for \(\beta(z + {\varepsilon_c} \delta)\) corresponds to
%   \[
%     \begin{cases}
%       \splitfrac{\mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, }{C(c_i + {\varepsilon_c}\delta), \mathcal{C}_k, \dots, \mathcal{C}_m} & \text{if } \delta = 1 \,,   \\
%       \splitfrac{\mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots,}{ \mathcal{C}_k, C(c_k + {\varepsilon_c}\delta), \dots, \mathcal{C}_m} & \text{if } \delta = -1 \, . \\
%       % \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, \underbrace{\mathcal{C}_i \cup \mathcal{C}_k}_{C(c_i + {\varepsilon_c}\delta)}, \dots, \mathcal{C}_m & \text{if } \delta = 0.  \\
%     \end{cases}
%   \]
%   \mm{I don't get why $\mathcal{C_k}$ appears with $C$, isn't it contained in $C$ ?}
% \end{remark}
%, defined as
% \(c^{(k)} = \{c_1, c_2, \dots, c_m\} \setminus c_k\), such that
% \[
%   c_i^{(k)} =
%   \begin{cases}
%     c_i     & \text{if } i < k,    \\
%     c_{i+1} & \text{if } i \geq k.
%   \end{cases}
% \]

We can now state the directional derivative of $\phi$. % \(J_k\).

\begin{theorem}\label{thm:sl1-directional-derivative}
  Let $\varepsilon_c$ be an arbitrary positive value such that
  \begin{equation}
    \label{eq:epsilon-c}
    \varepsilon_c < \big| c_i - c_j\big| , \quad \forall\, i \neq j \text{ and } \varepsilon_c < c_m \text{ if } c_m \neq 0 \, .
  \end{equation}
  Let \(c^{\setminus k}\) be the \(m - 1\) length version of $c$ where the $k$-th coordinate has been removed: $c^{\setminus k} = (c_1, \ldots c_{k-1}, c_{k+1}, \ldots, c_m)$.
  % Let \({\varepsilon_c} > 0\) satisfying \eqref{eq:epsilon-c}.
  The directional derivative of the partial sorted $\ell_1$ norm with respect to the $k$-th cluster \(\phi\), in the direction \(\delta\) is
  \[
    \phi'(z; \delta) =
    \begin{cases}
      \smashoperator[r]{\sum_{j \in C(\varepsilon_c )}} \lambda_{(j)^-_{\varepsilon_c }}
       & \text{if } z = 0 \, ,               \\
      \sign(z)\delta\smashoperator{\sum_{j \in C(z + {\varepsilon_c} \delta)}} \lambda_{(j)^-_{z + {\varepsilon_c}\delta}}
       & \text{if } |z| = c^{(k)}_i > 0 \, , \\
      \sign(z)\delta\smashoperator{\sum_{j \in C(z)}} \lambda_{(j)^-_{z}}
       & \text{otherwise} \, .
    \end{cases}
  \]
\end{theorem}

\begin{proof}
  Let $z$ such that $|z|$ is not one of the $c_i$'s and $z \neq 0$.
  Then, for any $h$ smaller that $\dist(z, \{c_i\} \cup \{0\})$, the permutation for $z$, $z - h$ and $z + h$ are all equal to:
  \begin{equation*}
    \mathcal{C}_1, \dots, \mathcal{C}_{k-1}, \mathcal{C}_{k+1}, \dots, \mathcal{C}_i, \mathcal{C}_k, \mathcal{C}_{i+1}, \dots, \mathcal{C}_m
  \end{equation*}


  If $z = 0$, then for $h$ small enough, $C(h)$ is the last/smallest cluster.


  Finally if $z = c_i$, for $h$ small enough the permutation for $\beta(z + h)$ is:
  \begin{equation}
    \mathcal{C}_1, \dots, \mathcal{C}_k, \mathcal{C}_{i}, \dots, \mathcal{C}_m
  \end{equation}
  while the one for $\beta(z - h)$ is:
  \begin{equation}
    \mathcal{C}_1, \dots, \mathcal{C}_{i}, \mathcal{C}_k, \dots, \mathcal{C}_m \, .
  \end{equation}

\end{proof}

In \Cref{fig:directional-derivative}, we show an example of the directional
derivative and the objective function.

\begin{figure}[htb]
  \centering
  \includegraphics[]{directional-derivative.pdf}
  \caption{%
  The function \(G\) and its directional derivative \(G( \cdot ; \delta)\) for
  an example with \(\beta = [-3.0, 1.0, 3.0, 2.0]^T\), \(k = 1\), and consequently
  \(c^{\setminus k} = [2, 1]^T\). The solution of \Cref{pb:cluster-problem} is the value of \(z\) for
      which \(P'_k(z; \delta) \geq 0 \) for \(\delta \in \{-1, 1\}\), which holds only
      at \(z = 2\), which must therefore be the solution.
    }
  \label{fig:directional-derivative}
\end{figure}

Using the directional derivative, we can introduce the SLOPE thresholding operator.

\begin{theorem}[The SLOPE Thresholding Operator]
  \label{thm:thresholding-operator}
  Define \(S(x) = \sum_{j \in C(x)}\lambda_{(j)^-_{x}}\) and
  let
  \[
    \begin{multlined}
      T_k(\gamma; \omega, c, \lambda) = \\
      \begin{cases}
        0
            & \text{if } |\gamma| \leq S(\varepsilon_c),               \\
        \sign(\gamma)c_i
            & \text{if } \omega c_i + S(c_i - \varepsilon_c)           \\
            & \quad \leq |\gamma| \leq                                 \\
            & \quad \omega c_i + S(c_i + \varepsilon_c),               \\
        \frac{\sign(\gamma)}{\omega} \big( |\gamma| - S(c_i + \varepsilon_c) \big)
            & \text{if } \omega c_i + S(c_i + {\varepsilon_c})         \\
            & \quad < |\gamma| <                                       \\
            & \quad \omega c_{i - 1} + S(c_{i - 1} - {\varepsilon_c}), \\
        \frac{\sign(\gamma)}{\omega} \big( |\gamma| - S(c_1 + {\varepsilon_c}) \big)
            & \text{if } |\gamma| \geq                                 \\
            & \quad \omega c_1 + S(c_1 + {\varepsilon_c}).
      \end{cases}
    \end{multlined}
  \]
  with \({\varepsilon_c}\) defined as in \eqref{eq:epsilon-c} and let
  \(\gamma = \tilde{r}^Tx\), \(\omega = \tilde{x}^T\tilde{x}\). Then
  \(T(\gamma; \omega, c^{(k)}, \lambda) \in \argmin_{z \in \mathbb{R}} P_k(z)\).
\end{theorem}

\jl{Consider adding a remark showing that our operator generalizes the soft thresholding operator.}

In \Cref{fig:slope-thresholding}, we visualize the SLOPE thresholding operator.

\begin{figure*}[htb]
  \centering
  \includegraphics[]{slope-thresholding.pdf}
  \caption{%
  An example of the SLOPE thresholding operator. The result corresponds to an
  example for \(\beta = [0.5, -0.5, 0.3, 0.7]^T\), \(c = [0.7, 0.5, 0.3]^T\)
  with an update for the second cluster (\(k = 2\)), such that
  \(c^{\setminus k} = [0.5, 0.3]^T\). Across regions where the function is constant,
      the operator sets the result to be either exactly 0 or to the value of one
      of the elements of \(c^{\setminus k}\).
    }
  \label{fig:slope-thresholding}
\end{figure*}

\subsubsection{Naive Updates}

As in \textcite{friedman2010}, we can improve the efficiency of updates by observing that
\begin{equation*}
  \begin{aligned}
    \tilde r_k & = y - \tilde y_k                                                                       \\
               & = y - X_{\bar{\mathcal{C}}_k}\beta_{\bar{\mathcal{C}}_k} - \tilde x c_k + \tilde x c_k \\
               & = r + \tilde x c_k
  \end{aligned}
\end{equation*}
and therefore that
\begin{equation}
  \label{eq:naive-update}
  \tilde x_k^T (y - \tilde y_k) = \tilde x_k^T r + \tilde x_k^T \tilde x_k c_k.
\end{equation}

\subsubsection{Caching Reductions}

Observe that \(\tilde x_k\) only changes between subsequent coordinate updates provided that the members of the cluster \(k\) change, for instance if two clusters are merged, a predictor leaves a cluster, or the signs flip (through an update of \(\alpha_k\)).
As a result, it is possible to obtain computational gains by caching \(\tilde x_k\) and \(\tilde x_k^T \tilde x_k\) for each cluster (except the zero cluster, which we do not consider in our coordinate descent step).
When there is no change in the clusters, there is no need to recompute these quantities.
And even when there are changes, we can still reduce the costs involved since \(\tilde x_k\) can be updated in place.
If a large cluster is joined by few new predictors, then the cost of updating may be much lower than recomputing the quantities for the entire cluster.
Also note that, for single-member clusters we only need to store \(\tilde x_k^T \tilde x_k\) since \(\tilde x_k\) is just a column in \(X\) times the corresponding sign.

Letting \(\tilde x_k^\text{old}\) correspond to the value of \(\tilde x_k\) before the update, we note that \(\tilde x_k \gets \tilde x_k^\text{old} + x_j \sign(\beta_j)\) for each \(j \in \mathcal{C}_k^\text{new} \setminus \mathcal{C}_k^\text{old}\) and \(\tilde x_k \gets \tilde x_k^\text{old} - x_j \sign(\beta_j)\) for each \(j \in \mathcal{C}_k^\text{old} \setminus \mathcal{C}_k^\text{new}\).
If only the signs flip, we simply have to also flip the signs in \(\tilde x_k\).

\subsubsection{Covariance Updates}

Notice that we can rewrite the first term in \eqref{eq:naive-update} as
\begin{equation}
  \begin{aligned}
    \tilde x_k^T r & = \tilde x_k^T y - \sum_{j : \beta_j \neq 0} \tilde x_k^T x_j \beta_j                                                                    \\
                   & = \tilde x_k^T y - \sum_{j : c_j \neq 0} \tilde x_k^T \tilde x_j c_j                                                                     \\
                   & = s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T y - \sum_{j : \beta_j \neq 0} s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T x_j \beta_j              \\
                   & = s_{\mathcal{C}_k}^T \left(X^T y\right)_{\mathcal{C}_k} - \sum_{j : \beta_j \neq 0} s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T x_j \beta_j \\
                   & = \sum_{j \in \mathcal{C}_k}\left( s_j x_j^Ty - \sum_{t : \beta_t \neq 0} s_j x_j^T x_t \beta_t \right)
  \end{aligned}
\end{equation}
As in \textcite{friedman2010}, this formulation can be used to achieve so-called \emph{covariance updates}.
We compute \(X^T y\) once at the start.
Then, each time a new predictor becomes non-zero, we compute its inner product with all other predictors, caching these products.

\subsection{Hybrid proximal coordinate descent strategy}

We propose an iterative solver that alternates between proximal gradient step and proximal coordinate descent.
Since the regularization term for SLOPE is not separable, applying PCD does not guarantee convergence.
However, \cite{dupuis2021} showed that once the clusters are known, the subdifferential of $J$ can be written as the cartesian product of the subdifferential of $J$ restricted to the clusters.
Hence, if one knew the clusters, PCD updates could be applied on each cluster.

The notion of clusters for SLOPE extends the notion of sparsity coming from the LASSO.
Identification of the sparsity pattern throughout the iterative algorithm have largely been studied.
Talk about Partly Smooth functions, related Manifold and that support transpose to cluster for SLOPE regularization. DO the maths.

Then identification of this underlying structure occurs when applying PGD.
Hence the idea to alternate, PGD and PCD step to take advantage of the speed of PCD and ensure convergence via the identification of the right structure with PGD steps.

\begin{algorithm}[htb]
  \SetKwInOut{Init}{init}
  \SetKwInOut{Input}{input}
  \caption{%
    Hybrid coordinate descent and proximal gradient descent algorithm
    for SLOPE\label{alg:hybrid}}
  \Input{
    \(X \in \mathbb{R}^{n\times p}, y\in \mathbb{R}^n, \lambda \in \{\mathbb{R}^p : \lambda_1 \geq \lambda_2 \geq \cdots > 0\}\), \(v \in \mathbb{N}\)}

  \Init{\(t \gets 0\), \(\beta \gets 0\), \(L \gets \lVert X \rVert_2^2\)}

  \Repeat{convergence}{
  \(t \gets t + 1\)

  \If{\(t \bmod v = 0\)}{
    \(\beta \leftarrow \operatorname{prox}_{J}(\beta - \frac{1}{L}\nabla f(\beta); \lambda / L)\) \label{alg:hybrid-istastep}

    Update \(c\), \(\mathcal{C}\)
  }
  \Else{
    \(k \gets 0\)

    \While{\(k \leq \lvert \mathcal{C} \rvert\)}{
      \(k \gets k + 1\)

      \(s \leftarrow \mathrm{sign}(\beta_{\mathcal{C}_k})\)

      \(\tilde x_k \gets X_{\mathcal{C}_k}s\)

      % \(\tilde y_k \gets X_{\widebar{\mathcal{C}}_k}\beta_{\widebar{\mathcal{C}}_k}\)

      \(\tilde r_k \gets y - X_{\widebar{\mathcal{C}}_k}\beta_{\widebar{\mathcal{C}}_k}\)

      % \(\gamma \gets \tilde{r}^T \tilde{x}_k\)

      % \(\omega \gets \tilde{x}_k^T \tilde{x}_k\)

      \(\beta \gets T(\tilde{r}^T \tilde{x}_k; \tilde{x}_k^T \tilde{x}_k, c^{\setminus k}, \lambda)\)

      Update \(c\), \(\mathcal{C}\)
    }
  }

  }
  \Return{\(\beta\)}
\end{algorithm}

In \Cref{fig:illustration-solver}, we show how \Cref{alg:hybrid} works in practice on
a two-dimensional SLOPE problem.

\begin{figure*}[htb]
  \centering
  \includegraphics{illustration_solvers}
  \caption{Illustration of the proposed solver. The figures show progress
    until convergence for the coordinate descent (CD) solver that we use as part
    of the hybrid method, our hybrid method, and  proximal gradient descent
    (PGD). The orange cross marks the optimum. Dotted lines indicate where the
    coefficients are equal in absolute value. The dashed lines indicate PGD
    steps and solid lines CD steps. Each dot marks a complete epoch, which may
    correspond to only a single coefficient update for the CD and hybrid
    solvers if the coefficients flip order. Each solver was run until the duality
    gap was smaller than \(10^{-10}\). Note that the CD algorithm cannot split clusters
    and is therefore stuck after the third epoch. The hybrid and PGD algorithms,
    meanwhile, reach convergence after 67 and 156 epochs respectively.}
  \label{fig:illustration-solver}
\end{figure*}

\begin{lemma}
  \label{lem:convergence}
  Let \(\beta^{(1)}, \beta^{(2)}, \dots, \beta^{(k)}\) be a sequence of
  iterates generated by \Cref{alg:hybrid}, \(1/v\) the frequency of proximal gradient
  descent iterates in \Cref{alg:hybrid}, and \(L\) the Lipschitz constant of
  \(\nabla F\). Then
  \[
    P(\beta^{(k)}) - P(\beta^*) \leq \frac{L \lVert \beta^{(0)} - \beta^* \rVert_2^2}{2\lfloor k/v \rfloor }\, ,
  \]
  where \(\beta^* \in \argmin_{\beta \in \mathbb{R}^p} P.\)
\end{lemma}


\section{Experiments}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The proposed algorithm is part of a python package relying on \pkg{numpy}~\parencite{harris2020} and \pkg{numba}~\parencite{lam2015}.
It will be made open-source upon publication. To compare its efficiency, we used several public datasets described in \Cref{table:real-data}.
We performed an extensive benchmark with the following competitors:
\begin{itemize}[noitemsep]
  \item Alternating Direction Method of Multipliers (ADMM)~\parencite{boyd2010}
  \item Anderson acceleration for proximal gradient descent~\parencite{zhang2020}
  \item Proximal Gradient Descent (PGD)~\parencite{combettes2005}
  \item Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)~\parencite{beck2009}
  \item Semismooth Newton-Based Augmented Lagrangian (Newt-ALM)~\parencite{Ziyan2019}
  \item The oracle solver uses the clusters obtained via another
        solver to compute coordinate descent updates from the known solver.
  \item The hybrid (our) solver (see \Cref{alg:hybrid}) combines proximal gradient descent
        and coordinate descent to overcome the non-separability of the SLOPE problem.
\end{itemize}

\begin{table}[hbt]
  \centering
  \caption{List of real data sets used in our experiments}
  \label{table:real-data}
  \begin{tabular}{
      l
      S[table-format=5.0,round-mode=off]
      S[table-format=7.0,round-mode=off]
      S[table-format=1.5,round-mode=figures,round-precision=2]
    }
    \toprule
    Dataset            & \(n\) & \(p\)   & {Density} \\ \midrule
    \dataset{Rhee2006} & 842   & 360     & 0.02469   \\
    \dataset{bcTCGA}   & 536   & 17322   & 1         \\
    \dataset{rcv1}     & 20242 & 44504   & 0.00166   \\
    \dataset{news20}   & 19996 & 1355191 & 0.0003357 \\ \bottomrule
  \end{tabular}
\end{table}

We used the \pkg{benchopt}~\parencite{moreau2022benchopt} tool to obtain the convergence curves for the different solvers.
\pkg{Benchopt} launches each solver several times increasing the number of iterations and store the objective value, dual gap and time to reach it.
The repository to reproduce the benchmark is available at XXX.
The datasets used for the experiments have been described in \Cref{table:datasets} and were obtained from \textcite{chang2011,chang2016} and \textcite{breheny2022}.

Unless we note otherwise, we use the Benjamini--Hochberg method to compute the \(\lambda\) sequence~\parencite{bogdan2015} with \(q=0.1\) to control the shape of the sequence.
We let \(\lambda_\text{max}\) be the \(\lambda\) sequence such that \(\beta^* = 0\), but for which any scaling with a strictly positive scalar smaller than one produces a solution with at least one non-zero coefficient.
We then parameterize the experiments by scaling \(\lambda_\text{max}\), using the fixed values \(0.25 \lambda_\text{max}\), \(0.1 \lambda_\text{max}\), \(0.02 \lambda_\text{max}\), which covers the range of very sparse solutions to the almost-saturated case.

\subsection{Simulated Data}

The design matrix $X$ is simulated with correlation between features $j$ and $j'$ equal to $\rho^{|j-j'|}$ where $\rho$ is a parameter that can be chosen in $[0, 1[$.
The true regression vector $\beta^\star$ contains a certain number of non-zero entries that are obtained by simulating a Gaussian distribution with zero mean and unit variance.
The observations are equal to $y=X\beta^\star + \varepsilon$ where $\varepsilon$ is a centered Gaussian distribution with variance such that $\lVert X\beta^\star\rVert / \lVert \varepsilon \rVert = 3$.

\begin{table}[hbt]
  \centering
  \caption{Settings for the simulated data in our benchmarks}
  \label{table:simulated-data}
  \begin{tabular}{
      S[table-format=5.0,round-mode=off]
      S[table-format=7.0,round-mode=off]
      S[table-format=2.0,round-mode=off]
      S[table-format=1.3,round-mode=off]
    }
    \toprule
    \(n\) & \(p\)   & \(k\) & {Density} \\ \midrule
    200   & 20000   & 20    & 1         \\
    20000 & 200     & 40    & 1         \\
    200   & 2000000 & 20    & 0.001     \\ \bottomrule
  \end{tabular}
\end{table}


\begin{figure*}[htb]
  \centering
  \includegraphics[scale=0.47]{simulated_legend.pdf}
  \includegraphics[scale=0.5]{simulated.pdf}
  \caption{\textbf{Benchmark on simulated datasets.} Normalized duality gap as a function of time for SLOPE on multiple simulated datasets and for multiple sequence of $\lambda$.}
  \label{fig:simulated}
\end{figure*}

In \Cref{fig:simulated}, we present the results of the benchmarks on simulated data.
The settings for the simulated can be found in \Cref{table:datasets}.
We see that for smaller fractions of $\lambda_{\text{max}}$ our hybrid algorithm allows significant speedup in comparison to its competitors mainly when the number of features is large than the number of samples.
On very large scale data such as in simulated data setting $3$, we see that the hybrid solver is faster than its competitors by one or two orders of magnitude.


\subsection{Real Data}
\begin{figure*}[htb]
  \centering
  \includegraphics[scale=0.47]{real_legend.pdf}
  \includegraphics[scale=0.5]{real.pdf}
  \caption{\textbf{Benchmark on real datasets.} Normalized duality gap as a function of time for SLOPE on multiple simulated datasets and for multiple sequence of $\lambda$.}
  \label{fig:real}
\end{figure*}

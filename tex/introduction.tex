%!TEX root = ./slopecd.tex
\section{Introduction}\label{sec:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper we present a novel numerical algorithm for Sorted L-One Penalized
Estimation (SLOPE)~\cite{bogdan2013, bogdan2015}, defined as
\begin{equation}
  \label{eq:slope-problem}
  \operatorname{minimize}_{\beta \in \mathbb{R}^p}
  P(\beta) = L(\beta) + J(\beta)
\end{equation}
where we take \(L\) to be smooth and twice differentiable and
\begin{equation}
  \label{eq:sorted-l1-norm}
  J(\beta) = \sum_{j=1}^p \lambda_j|\beta_{(j)}|
\end{equation}
is the \emph{sorted \(\ell_1\) norm}, defined such that
\[
  |\beta_{(1)}| \geq |\beta_{(2)}| \geq \cdots \geq |\beta_{(p)}|,
\]
and \(\lambda\) is a fixed non-increasing and non-negative sequence.

SLOPE is a sparse regression method that has become increasingly popular due to
several appealing properties, such as its ability to control false discovery
rate~\cite{bogdan2015, kos2020}, cluster coefficients~\cite{figueiredo2016,
schneider2020a}, and recover sparsity and ordering patterns in the
solution~\cite{bogdan2022}. Unlike other competing regularization methods such
as MCP~\cite{zhang2010} and SCAD~\cite{fan2001}, SLOPE is also a convex
problem~\cite{bogdan2015}.

In spite of the availability of predictor screening rules~\cite{elvira2022,
larsson2020c}, which help speed up SLOPE in the high-dimensional regime,
current state-of-the-art algorithms for SLOPE perform poorly in comparison to
those of other, more estabilished, penalization methods such as the lasso
(\(\ell_1\)-norm regularization) and ridge regression (\(\ell_2\)-norm
regularization). As a small illustration of this issue, we compared the
speed at which the SLOPE and glmnet packages fit a complete regularization
path for the bcTCGA data set. SLOPE takes x seconds to fit the full path,
whilst glmnet requires only y seconds.

This lackluster performance has hampered the applicability of SLOPE to many
real-world applications, particularly because users often need to solve SLOPE
repeatedly when tuning hyper-parameters for SLOPE, for instance in
cross-validation, or when fitting SLOPE as part of Adaptive Bayesian
SLOPE~\cite{jiang2022}.

A major reason for why algorithms for solving other regression models with
non-differentiable penalties, such as the lasso, MCP, and SCAD, enjoy better
performance is that they minimize their objectives using coordinate
descent~\cite{breheny2011, friedman2010} rather than proximal gradient descent
algorithms such as FISTA~\cite{beck2009} and the
alternating direction method of multipliers method (ADMM)~\cite{boyd2010},
which are both used in the current version of the SLOPE package.
\jl{Add benchopt reference.}

Applying coordinate descent to SLOPE is, however, not quite straightforward
because convergence guarantees for coordinate descent require the
objective to be separable, which, as we can see from \eqref{eq:sorted-l1-norm}
is not the case for SLOPE. 

% The resulting solution \(\hat{\beta}\) has the property that it can cluster the
% coefficients by their magnitudes, such that \(\mathcal{C}=
% \{j: |\beta_j|=c\}\) for some \(c\).

In this article we address this problem by introducing a new, highly effective
algorithm for SLOPE based on a hybrid proximal gradient and coordinate descent
scheme. Our method features convergence guarantees and reduces the time
required to fit SLOPE by orders of magnitude in our empirical experiments.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.62]{illustration_solvers.pdf}
  \caption{Illustration of proposed solver.}
  \label{fig:illustration-solver}
\end{figure}
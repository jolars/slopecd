
Penalized regression has become a core element of modern statistical learning. 
Among the numerous existing methods, the lasso is the most famous estimator allowing to perform feature selection due to the sparse solution induced by the $\ell_1$ penalty. 
The success of the lasso is related to the speed at which the underlying optimization problem can be solved, state-of-the-art solvers relying on the well known coordinate descent algorithm. 
Despite its interesting statistical properties, the Sorted L-One Penalized Estimation (SLOPE), a generalization of the lasso, has not yet reached a wide interest. 
One of the main reason is due to the difficulty to solve the underlying optimization problem efficiently. 
In this work, we propose a new fast algorithm to solve the SLOPE optimization problem. 
Despite the non-separability of the penalty, we propose a hybrid method that combines proximal gradient descent steps and proximal coordinate descents ones. 
We provide new results on the directional derivatives of the SLOPE penalty function, its related SLOPE thresholding operator and convergence guarantees for our proposed solver.
To highlights the speed of our method, we performed an extensive benchmark based on simulated and real datasets including a large list of competitors.

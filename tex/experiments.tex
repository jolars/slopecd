\section{EXPERIMENTS}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To investigate the performance of our algorithm, we performed an extensive benchmark against the following competitors:
\begin{itemize}[noitemsep]
  \item Alternating Direction Method of Multipliers (\texttt{ADMM})~\parencite{boyd2010}
  \item Anderson acceleration for proximal gradient descent (\texttt{anderson PGD})~\parencite{zhang2020}
  \item Proximal Gradient Descent (\texttt{PGD})~\parencite{combettes2005}
  \item Fast Iterative Shrinkage-Thresholding Algorithm (\texttt{FISTA})~\parencite{beck2009}
  \item Semismooth Newton-Based Augmented Lagrangian (\texttt{Newt-ALM})~\parencite{Ziyan2019}
  \item The Oracle solver (\texttt{oracle CD}) uses the clusters obtained via another
        solver to compute coordinate descent updates from the known solver.
  \item The hybrid (our) solver (see \Cref{alg:hybrid}) combines proximal gradient descent
        and coordinate descent to overcome the non-separability of the SLOPE problem.
\end{itemize}

We used the \pkg{benchopt}~\parencite{moreau2022benchopt} tool to obtain the convergence curves for the different solvers.
\pkg{Benchopt} is a collaborative framework that allows reproducible and automatic benchmarks.
% This would void anonymity. We should uncomment for the camera-ready version.
% The repository to reproduce the benchmark is available at \url{https://github.com/Klopfe/benchmark_slope}.
The repository to reproduce the benchmark is available in the supplementary material.

Unless we note otherwise, we used the Benjamini--Hochberg method to compute the \(\lambda\) sequence~\parencite{bogdan2015},
which sets $\lambda_j = \eta^{-1}(1 - q\times j / (2p))$ for $j=1, 2, \hdots, p$ where $\eta^{-1}$ is the probit function.
For the rest of the experiments section, the parameter $q$ of this sequence has been set to $0.1$ if not stated otherwise.
We let \(\lambda_\text{max}\) be the \(\lambda\) sequence such that \(\beta^* = 0\), but for which any scaling with a strictly positive scalar smaller than one produces a solution with at least one non-zero coefficient.
We then parameterize the experiments by scaling \(\lambda_\text{max}\), using the fixed values \(0.5 \lambda_\text{max}\), \(0.1 \lambda_\text{max}\), \(0.02 \lambda_\text{max}\), which covers the range of very sparse solutions to the almost-saturated case.

We pre-process datasets by first removing features with less than three non-zero values. Then, for dense data we center and scale each feature by its mean and standard deviation respectively.
For sparse data, we scale each feature by its maximum absolute value.

Each solver was coded in \pkg{python}, using \pkg{numpy}~\parencite{harris2020} and \pkg{numba}~\parencite{lam2015} for performance-critical code.
The code will be made open-source upon publication.

\jl{We are going to include it in the supplementary material too, right?}

\jl{Add information about the benchmarking system here (or refer to entry in appendix).}

\subsection{Simulated Data}
\label{sec:experiments-real-data}

The design matrix $X$ was generated such that features had mean one and unit variance, with correlation between features $j$ and $j'$ equal to $0.6^{|j-j'|}$.
%where $\rho$ is a parameter that can be chosen in $[0, 1[$.
%We fixed this value at $0.6$.
We generated \(\beta \in \mathbb{R}^p\) such that \(k\) entries, chosen uniformly at random throughout the vector, were sampled from a standard Gaussian distribution.
The response vector, meanwhile, was set to $y=X\beta + \varepsilon$, where
$\varepsilon$ was sampled from a multivariate Gaussian distribution with variance such that $\lVert X\beta\rVert / \lVert \varepsilon \rVert = 3$.

The different scenarios for the simulated data are described in \Cref{tab:simulated-data}

\begin{table}[hbt]
  \centering
  \caption{Scenarios for the simulated data in our benchmarks}
  \label{tab:simulated-data}
  \begin{tabular}{
      l
      S[table-format=5.0,round-mode=off]
      S[table-format=7.0,round-mode=off]
      S[table-format=2.0,round-mode=off]
      S[table-format=1.3,round-mode=off]
    }
    \toprule
    {Scenario} & {\(n\)} & {\(p\)} & {\(k\)} & {Density} \\ \midrule
    1          & 200     & 20000   & 20      & 1         \\
    2          & 20000   & 200     & 40      & 1         \\
    3          & 200     & 2000000 & 20      & 0.001     \\ \bottomrule
  \end{tabular}
\end{table}

In \Cref{fig:simulated}, we present the results of the benchmarks on simulated data.

\begin{figure*}[!t]
  \centering
  \includegraphics{simulated_legend.pdf}
  \includegraphics{simulated.pdf}
  \caption{Benchmark on simulated datasets. Normalized duality gap as a function of time for SLOPE on multiple simulated datasets and for multiple sequence of $\lambda$.}
  \label{fig:simulated}
\end{figure*}

We see that for smaller fractions of $\lambda_{\text{max}}$ our hybrid algorithm allows significant speedup in comparison to its competitors mainly when the number of features is larger than the number of samples.
On very large scale data such as in simulated data setting $3$, we see that the hybrid solver is faster than its competitors by one or two orders of magnitude.

\subsection{Real data}
\label{sec:experiments-real-data}

The datasets used for the experiments have been described in \Cref{tab:real-data} and were obtained from \textcite{chang2011,chang2016} and \textcite{breheny2022}.

\begin{table}[hbt]
  \centering
  \caption{List of real data sets used in our experiments}
  \label{tab:real-data}
  \begin{tabular}{
      l
      S[table-format=5.0,round-mode=off]
      S[table-format=7.0,round-mode=off]
      S[table-format=1.5,round-mode=figures,round-precision=2]
    }
    \toprule
    Dataset            & {\(n\)} & {\(p\)} & {Density} \\ \midrule
    \dataset{bcTCGA}   & 536     & 17322   & 1         \\
    \dataset{news20}   & 19996   & 1355191 & 0.0003357 \\
    \dataset{rcv1}     & 20242   & 44504   & 0.00166   \\
    \dataset{Rhee2006} & 842     & 360     & 0.02469   \\ \bottomrule
  \end{tabular}
\end{table}

\Cref{fig:real-data} shows the suboptimality for the objective function $P$ as a function of the time for the four different datasets.
We see that when the regularization parameter is set at $\lambda_{\text{max}}/2$ and $\lambda_{\text{max}}/10$ our proposed solver is faster than all its competitors especially when the datasets becomes larger.
This is especially visible on the \dataset{news20} dataset where we see that our proposed method is faster by at least one order of magnitude.

When the parametrization value is set to $\lambda_{\text{max}}/50$, our algorithm remains competitive on the different datasets.
It can be seen that the different competitors do not behave consistently accross the datasets.
For example, the \texttt{newt-alm} method is very fast on the \dataset{bcTCGA} dataset but is very slow on the \dataset{news20} dataset whereas the \texttt{hybrid} method remains very efficient in both settings.

\begin{figure*}[!t]
  \centering
  \includegraphics{real_legend.pdf}
  \includegraphics{real.pdf}
  \caption{Benchmark on real datasets. Normalized duality gap as a function of time for SLOPE on multiple simulated datasets and for multiple sequence of $\lambda$.}
  \label{fig:real-data}
\end{figure*}

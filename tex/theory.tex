%!TEX root = ./slopecd.tex

\section{Theory}\label{sec:theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Coordinate Descent for SLOPE}%
\label{sec:coordinate-updates}

In this section we derive a coordinate descent algorithm for minimizing the SLOPE problem~\eqref{eq:slope-problem} with respect to the coefficients of a single cluster at a time.
First observe that we can write \eqref{eq:slope-problem} as
\[
  \begin{aligned}
    P(\beta)
     & =  \frac{1}{2} \lVert y - X\beta\rVert_2^2 + J(\beta)                        \\
     & = \frac{1}{2} \lVert y - X_{\bar{\mathcal{C}_k}} \beta_{\bar{\mathcal{C}_k}}
    - \big(X_{\mathcal{C}_k} s_{\mathcal{C}_k}\big)c_k  \rVert_2^2
    + |c_k|\sum_{j \in {\mathcal{C}_k}} \lambda_{(j)^-}
    + \sum_{j \notin {\mathcal{C}_k}} |\beta_j|\lambda_{(j)^-}                      \\
     & = \frac{1}{2} \lVert \tilde r - \tilde x c_k \rVert_2^2
    + |c_k|\sum_{j \in {\mathcal{C}_k}} \lambda_{(j)^-}
    + \sum_{j \notin {\mathcal{C}_k}} |\beta_j|\lambda_{(j)^-}                      \\
  \end{aligned}
\]
with \( \tilde x_k = X_{\mathcal{C}_k} s_{\mathcal{C}_k}\) and
\(\tilde r_k = y - \tilde y_k = y - X_{\bar{\mathcal{C}}_k} \beta_{\bar{\mathcal{C}_k}}\).

We propose a coordinate-wise update that minimizes \(P(\beta)\) with respect to the
clusters' corresponding coefficients one at a time, keeping the relative signs
of the coordinates within each cluster fixed but allowing all of the signs to
flip (simultaneously).

Letting \(\beta\) correspond to a fixed vector, note that
\(\beta_i = s_i c_{(j)^-}\), \(i \in \mathcal{C}_j\) for all
\(i\). Now let \(\tilde{\mathcal{C}}_i\)
correspond to the indices of the \(i\)th cluster for \(\beta\), that is,
\(|\beta_j| = c_i\) for all \(j \in \tilde{\mathcal{C}}_i\).
Next, we let
\begin{equation*}
  \beta_i(z) =
  \begin{cases}
    s_k z   & \text{if } i \in \mathcal{C}_k, \\
    \beta_i & \text{otherwise.}
  \end{cases}
\end{equation*}
This means that \(\beta(x)\) corresponds to a version of \(\beta\) with a
coordinate update for the \(k\)th cluster.

This corresponds to solving the following
one-dimensional problem:

\begin{equation*}
  \operatorname*{minimize}_{z \in \mathbb{R}} \left( \frac{1}{2} \lVert \tilde r - \tilde x z \rVert_2^2 + J_k(z)\right)
\end{equation*}
with
\[
  J_k(z) = |z| \sum_{j \in \mathcal{C}_k} \lambda_{(j)^-_z}
  + \sum_{j \notin \mathcal{C}_k} |\beta_j| \lambda_{(j)^-_z},
\]
where we write \(\lambda_{(j)^-_z}\) to indicate that the permutation operator \((j)^-_z\)
depends on \(\beta(z)\).

The optimality condition for this problem is \(0 \in \partial_z P(\beta).\)
Differentiating with respect to \(z\) yields
\begin{equation}
  \label{eq:cluster-grad}
  \partial_{z} P(\beta) = \tilde x^T \tilde x z - \tilde r^T \tilde x + \partial_{z} J_k(z)
\end{equation}
where the last term is the partial subdifferential of the sorted \(\ell_1\)
norm with respect to coefficient of the \(k\)th cluster.

\subsubsection{Partial Subdifferential of the Sorted \(\ell_1\) Norm}

In this section we derive an expression for the partial subdifferential of the
sorted \(\ell_1\) norm. To do this, we will first introduce the directional
derivative for the sorted \(\ell_1\) norm with respect to the coefficient of
the \(k\)th cluster.

Before we arrive at the definition for the directional derivative, however,
we introduce a key fact in \cref{def:limit-permutation}.

\begin{definition}
  \label{def:limit-permutation}
  There exists an \(\varepsilon > 0\) such that
  \begin{equation}
    c_{i - 1} > c_i \pm h \delta > c_{i + 1} \geq 0 \qquad \forall\, h < \varepsilon, i,
  \end{equation}
\end{definition}

Note that for an \(\varepsilon\) defined as in \cref{def:limit-permutation}, we have
\begin{equation}
  \lim_{h \downarrow 0} C(z + h\delta) = C(z + \varepsilon \delta)
  \quad\text{and}\quad
  \lim_{h \downarrow 0} \lambda_{(i)^-_{z + h\delta}}
  = \lambda_{(i)^-_{z + \varepsilon\delta}}.
\end{equation}
In other words, the order permutation corresponding to \(B(z + h\delta)\) depends only on
\(\delta\) in the limit as \(h\) tends to~\(0\).

\begin{remark}
  A consequence of \cref{def:limit-permutation} is that the order
  permutations for \(B(z)\) and \(B(z + \varepsilon \delta)\) differ only for a subset of
  the permutation vectors. For \(z = 0\)
  these permutations correspond to
  \[
    \begin{cases}
      \tilde{\mathcal{C}}_1, \dots, \tilde{\mathcal{C}}_{k-1}, \tilde{\mathcal{C}}_{k+1}, \dots, \tilde{\mathcal{C}}_m, C(\varepsilon\delta)                                          & \text{if } c_m > 0 \text{ and } \delta \neq 0, \\
      \tilde{\mathcal{C}}_1, \dots, \tilde{\mathcal{C}}_{k-1}, \tilde{\mathcal{C}}_{k+1}, \dots, C(\varepsilon\delta), \tilde{\mathcal{C}}_m                                          & \text{if } c_m = 0 \text{ and } \delta \neq 0, \\
      \tilde{\mathcal{C}}_1, \dots, \tilde{\mathcal{C}}_{k-1}, \tilde{\mathcal{C}}_{k+1}, \dots, \underbrace{\tilde{\mathcal{C}}_k \cup \tilde{\mathcal{C}}_m}_{C(\varepsilon\delta)} & \text{if } c_m = 0 \text{ and } \delta = 0.    \\
    \end{cases}
  \]
  In the case when \(z = c_i \neq 0\), \(i \neq k\), the corresponding permutations are
  \[
    \begin{cases}
      \tilde{\mathcal{C}}_1, \dots, \tilde{\mathcal{C}}_{k-1}, \tilde{\mathcal{C}}_{k+1}, \dots, C(c_i + \varepsilon\delta), \tilde{\mathcal{C}}_i, \dots, \tilde{\mathcal{C}}_m                                          & \text{if } \delta = 1,  \\
      \tilde{\mathcal{C}}_1, \dots, \tilde{\mathcal{C}}_{k-1}, \tilde{\mathcal{C}}_{k+1}, \dots, \tilde{\mathcal{C}}_i, C(c_i + \varepsilon\delta), \dots, \tilde{\mathcal{C}}_m                                          & \text{if } \delta = -1, \\
      \tilde{\mathcal{C}}_1, \dots, \tilde{\mathcal{C}}_{k-1}, \tilde{\mathcal{C}}_{k+1}, \dots, \underbrace{\tilde{\mathcal{C}}_i \cup \tilde{\mathcal{C}}_k}_{C(c_i + \varepsilon\delta)}, \dots, \tilde{\mathcal{C}}_m & \text{if } \delta = 0.  \\
    \end{cases}
  \]
\end{remark}

We are now ready to define the directional derivative of \(J_k\)~(\cref{thm:sl1-directional-derivative}).

\begin{theorem}\label{thm:sl1-directional-derivative}
  The directional derivative of \(J_k\) in the direction \(\delta\) is
  \[
    J'_k(z, \delta) =
    \begin{cases}
      \sign(z)\delta\smashoperator{\sum_{j \in C(z + \varepsilon \delta)}}
      \lambda_{(j)^-_{z + \varepsilon\delta}}                      & \text{if } z = c_i > 0, \\
      |\delta|\smashoperator{\sum_{j \in C(\varepsilon \delta)}}
      \lambda_{(j)^-_{\varepsilon\delta}}                          & \text{if } z = 0,       \\
      |\delta| \smashoperator{\sum_{j \in C(z)}} \lambda_{(j)^-_z} & \text{otherwise}
    \end{cases}
  \]
  with \(\varepsilon\) defined as in \cref{def:limit-permutation}.
\end{theorem}
\begin{proof}
  The directional derivative of \(J_k\) in the direction \(\delta\) is defined as
  \begin{align}
    J_k'(z; \delta)
    = & \lim_{h \downarrow 0} \frac{J_k(z + h \delta) - J_k(z)}{h} \nonumber \\
    = &
    \lim_{h \downarrow 0} \frac{1}{h}
    \left(
      |z + h \delta| \smashoperator{\sum_{j \in C(z + h \delta)}} \lambda_{(j)^-_{z + h \delta}}
      + \smashoperator{\sum_{j \notin C(z + h\delta)}} |\beta_j| \lambda_{(j)^-_{z + h \delta}}
      - |z| \smashoperator{\sum_{j \in C(z)}} \lambda_{(j)^-_z}
      - \smashoperator{\sum_{j \notin C(z)}} |\beta_j| \lambda_{(j)^-_z}
    \right) \nonumber                                                        \\
    = & \lim_{h \downarrow 0}
    \frac{1}{h}
    \left(
      |z + h \delta| \smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z + \varepsilon\delta}}
      + \smashoperator{\sum_{j \notin C(z + \varepsilon\delta)}} |\beta_j| \lambda_{(j)^-_{z + \varepsilon\delta}}
      - |z| \smashoperator{\sum_{j \in C(z)}} \lambda_{(j)^-_{z}}
      - \smashoperator{\sum_{j \notin C(z)}} |\beta_j| \lambda_{(j)^-_{z}}
    \right).
    \label{eq:directional-derivative-sl1}
  \end{align}

  First, note that \(J_k\) is differentiable for \(z \notin \{0, c_i\}\), \(i
  \neq k\) and that \(C(z + \varepsilon\delta) = C(z)\), giving
  the directional derivative
  \[
    \delta \sign(z) \smashoperator{\sum_{j \in C(z)}} \lambda_{(j)^-_{z}}.
  \]

  Turning to the case of \(z = \pm c_i\), \(i \neq k\) now, note that we have,
  as a result of the definition of
  \(\varepsilon\)~(\cref{def:limit-permutation}), the following identities:
  \begin{align*}
    \tilde{\mathcal{C}}_i               & = \widebar{C(c_i + \varepsilon \delta)} \cap C(c_i),                                                                          \\
    C(c_i)                              & = \tilde{C}_i \cup \big(C(c_i + \varepsilon\delta) \cap C(c_i)\big) = \tilde{\mathcal{C}}_i \cup C(c_i + \varepsilon \delta), \\
    \widebar{C(z + \varepsilon \delta)} & = \widebar{C(c_i)} \cup \tilde{\mathcal{C}}_i.
  \end{align*}
  Using this, it turns out that we can rewrite \eqref{eq:directional-derivative-sl1}
  as
  \begin{equation*}
    \label{eq:directional-derivative-simplified}
    J'_k(z; \delta)
    = \lim_{h \downarrow 0} \frac{1}{h}
    \left(
      \splitfrac{
      |z + h\delta|\smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z + \varepsilon\delta}}
      + |c_i|\smashoperator{\sum_{j \in \tilde{C}_i}} \lambda_{(j)^-_{z + \varepsilon\delta}}
      + \smashoperator{\sum_{j \in \widebar{C(z)}}} |\beta_j|\lambda_{(j)^-_{z + \varepsilon\delta}}
      }{%
      - |z| \smashoperator{\sum_{j \in \tilde{\mathcal{C}}_i}} \lambda_{(j)^-_{z}}
      - |c_i| \smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z}}
      - \smashoperator{\sum_{j \in \widebar{C(z)}}} |\beta_j| \lambda_{(j)^-_{z}}
      }
    \right).
  \end{equation*}
  Next, observe that \(\lambda_{(j)^-_{z + \varepsilon\delta}} =
  \lambda_{(j)^-_{z}}\) for all \(j \in \widebar{C(z)}\) and consequently
  \[
    \smashoperator{\sum_{j \in \widebar{C(z)}}} |\beta_j|\lambda_{(j)^-_{z + \varepsilon\delta}} =
    \smashoperator{\sum_{j \in \widebar{C(z)}}} |\beta_j|\lambda_{(j)^-_{z}}.
  \]
  Moreover, note that, since \(z = \pm c_i\), there exists a permutation corresponding to
  \(\lambda_{(j)^-_{z}}\) such that
  \[
    |z|\smashoperator{\sum_{j \in \tilde{C}_i}} \lambda_{(j)^-_{z + \varepsilon\delta}}
    = |c_i| \smashoperator{\sum_{j \in \tilde{C}_i}} \lambda_{(j)^-_{z}}
  \]
  and consequently
  \begin{equation}
    \label{eq:directional-derivative-simplified-again}
    J'_k(z; \delta)
    = \lim_{h \downarrow 0} \frac{1}{h}
    \left(
      |z + h\delta|\smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z + \varepsilon\delta}}
      - |c_i| \smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z}}.
    \right).
  \end{equation}
  Now, since \(c_i + h \delta > 0\) and \(-c_i + h \delta < 0\) in the limit as
  \(h\) goes to \(0\) for \(c_i \neq 0\), we have
  \[
    \lim_{h\downarrow 0} |-c_i + h \delta|
    = \lim_{h\downarrow 0}\big( |c_i| -h \delta\big)
    \quad\text{and}\quad
    \lim_{h\downarrow 0} |c_i + h \delta|
    = \lim_{h\downarrow 0}(|c_i| + h \delta)
  \]
  which means that
  \begin{align*}
    J'_k(z; \delta)
     & = \lim_{h \downarrow 0} \frac{1}{h}
    \left(
      \big(|z| + \sign(z)h\delta\big)\smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z + \varepsilon\delta}}
      - |c_i| \smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z}}.
    \right)                                                                                                          \\
     & = \lim_{h \downarrow 0} \frac{1}{h}
    \sign(z)h\delta\smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z + \varepsilon\delta}}     \\
     & = \sign(z)\delta\smashoperator{\sum_{j \in C(z + \varepsilon\delta)}} \lambda_{(j)^-_{z + \varepsilon\delta}} \\
  \end{align*}

  In the case when \(z = c_i = 0\), that is, the cluster corresponding to
  zero coefficients in \(\beta\) is non-empty,
  then \eqref{eq:directional-derivative-simplified-again} is
  simply
  \begin{equation*}
    \label{eq:directional-derivative-zerocase}
    J'_k(0; \delta)
    = \lim_{h \downarrow 0} \frac{1}{h}
    |h\delta|\smashoperator{\sum_{j \in C(\varepsilon\delta)}} \lambda_{(j)^-_{\varepsilon\delta}}
    = |\delta|\smashoperator{\sum_{j \in C(\varepsilon\delta)}} \lambda_{(j)^-_{\varepsilon\delta}}.
  \end{equation*}

  Proceeding, now, with the case when \(z = 0 \neq c_i\), \(i \neq k\),
  we see that \eqref{eq:directional-derivative-sl1} reduces to
  \begin{equation*}
    J_k'(0; \delta) = \lim_{h \downarrow 0}
    \frac{1}{h}
    \left(
      |h \delta| \smashoperator{\sum_{j \in C(\varepsilon\delta)}} \lambda_{(j)^-_{\varepsilon\delta}}
      + \smashoperator{\sum_{j \notin C(\varepsilon\delta)}} |\beta_j| \lambda_{(j)^-_{\varepsilon\delta}}
      - \smashoperator{\sum_{j \notin C(0)}} |\beta_j| \lambda_{(j)^-_{0}}
    \right).
  \end{equation*}
  In this case, we have \(C(0) = C(\varepsilon\delta)\) since \(c_m \neq 0\), and therefore
  \[
    \smashoperator{\sum_{j \notin C(\varepsilon\delta)}} |\beta_j| \lambda_{(j)^-_{\varepsilon\delta}}
    = \smashoperator{\sum_{j \notin C(0)}} |\beta_j| \lambda_{(j)^-_{0}},
  \]
  which means that
  \begin{equation*}
    J_k'(0; \delta) = \lim_{h \downarrow 0}
    \frac{1}{h}
    |h \delta| \smashoperator{\sum_{j \in C(\varepsilon\delta)}} \lambda_{(j)^-_{\varepsilon\delta}}
    = |\delta|\smashoperator{\sum_{j \in C(\varepsilon\delta)}} \lambda_{(j)^-_{\varepsilon\delta}}
    = |\delta|\smashoperator{\sum_{j \in C(0)}} \lambda_{(j)^-_{0}}.
  \end{equation*}
\end{proof}

Using \cref{thm:sl1-directional-derivative}, we now define the subgradient for
\(J_k\), \(\partial J_k\) in \cref{thm:cluster-subdifferential}.

\begin{theorem}\label{thm:cluster-subdifferential}
  Let \(a(i)\) and \(b(i)\) point to the first and last indices, respectively,
  for the \(\lambda\) sequence associated with the \(i\)th cluster
  \(\mathcal{C}_i\) corresponding to the vector \(B(z)\). Let \(p_k\) be the
  size of \(k\)th cluster and \(c_i\) the coefficients of the \(i\)th cluster
  for a fixed \(\beta\). The subdifferential for \(J_k(z)\) at \(z\) is then
  \[
    \partial_{z} J_k(z) =
    \begin{cases}
      \big[-\sum_{j = p - p_k + 1}^{p}\lambda_j, \sum_{j = p - p_k + 1}^{p}\lambda_j\big]                                                                         & \text{if } z = 0 \neq c_m,  \\
      \big[-\sum_{j = p - |\mathcal{C}_m| + 1}^{p - |\mathcal{C}_m| + p_k}\lambda_j, \sum_{j = p - |\mathcal{C}_m| + 1}^{p - |\mathcal{C}_m| + p_k}\lambda_j\big] & \text{if } z = 0 = c_m,     \\
      \big[\sum_{j = b(i) - p_k + 1}^{b(i)}\lambda_j, \sum_{j = a(i)}^{a(i) + p_k} \lambda_j\big]                                         & \text{if } z = c_i \neq 0,  \\
      \big[-\sum_{j = a(i)}^{a(i) + p_k} \lambda_j, -\sum_{j = b(i) - p_k + 1}^{b(i)}\lambda_j\big]                                       & \text{if } z = -c_i \neq 0, \\
      \sign(z)\sum_{j \in C(z)} \lambda_{(j)^-_z}                                                                                                                 & \text{otherwise.}           \\
    \end{cases}
  \]
\end{theorem}
\begin{proof}
  Since \(J_k(z)\) is convex, \(g \in \mathbb{R}\) is a
  subgradient at \(z\) if and only if~\cite[Theorem 23.2]{rockafellar1970}
  \[
    J'(x; \delta) \geq g\delta \qquad \forall\,\delta \in \{-1, 1\}.
  \]
\end{proof}

The objective and subgradient for the cluster-wise problem are shown in
\cref{fig:cluster-grad-obj}.

\begin{figure}[htbp]
  \centering
  \includegraphics[]{clusterupdate-grad-obj}
  \caption{%
    Objective and gradient under the constraint that we have a fixed
    cluster.
    The optimum is found at \(\alpha = 0.3\).
  }%
  \label{fig:cluster-grad-obj}
\end{figure}

Let \(\mathcal{B}\) be a cluster initialized to \(\mathcal{C}_k\), with
corresponding coefficient \(c_k\).
Then the coordinate update for \(\beta_\mathcal{B}\) is\jl{This obviously
  does not work when \(s=0\). Can we just set \(s = 1\) here? It's just
  the relative signs that really matter, right?}\jw{That is a really good point. Do you think we can tie to the sign of the gradient instead?. We also will not move the zero cluster this way suspect. But it should work for any formation then sign of the gradient is the way I think?}
\jl{Yes, I think you're right, in fact that's what I have in the code now.}
\[
  \beta_\mathcal{B} \gets
  s_\mathcal{B}
  T \left(
  \frac{\tilde r^T \tilde x}{\tilde x^T \tilde x},
  \frac{\lambda}{\tilde x^T \tilde x},
  k,
  \tilde \beta,
  \mathcal{C}
  \right)
\]
where

\begin{equation}
  \label{eq:slope-thresholding}
  T(a, \lambda,k,\tilde \beta, \mathcal{C}) =
  \begin{cases}
    0                                                                 & \text{if } |a| \leq \sum_{i=1}^{|\mathcal{C}_k|}\lambda^{C_0}_i                                                                                                                               \\
    \sign(a)|\tilde \beta_i|                                          & \text{if } \sum_{j= |\mathcal{C}_i| - |\mathcal{C}_k| + 1}^{|\mathcal{C}_i|} \lambda^{\mathcal{C}_i}_j \leq |a| - |\tilde \beta_i| \leq \sum_{j=1}^{|\mathcal{C}_k|}\lambda^{\mathcal{C}_i}_j \\
    \sign(a)\big(|a| - \sum_{j \in \mathcal{C}_k}\lambda_{(j)^-}\big) & \text{otherwise.
    }
  \end{cases}
\end{equation}
To emphasize the connection between \(T\) and the soft-thresholding operator
for the lasso, we call this operator the SLOPE-thresholding operator.
In \cref{fig:slope-thresholding}, we visualize the operator.

\begin{figure}[htbp]
  \centering
  \includegraphics[]{slope-thresholding.pdf}
  \caption{The result of the SLOPE thresholding update.}
  \label{fig:slope-thresholding}
\end{figure}

\subsubsection{Naive Updates}

As in \textcite{friedman2010}, we can improve the efficiency of updates by observing that
\begin{equation*}
  \begin{aligned}
    \tilde r_k & = y - \tilde y_k                                                                       \\
               & = y - X_{\bar{\mathcal{C}_k}}\beta_{\bar{\mathcal{C}_k}} - \tilde x c_k + \tilde x c_k \\
               & = r + \tilde x c_k
  \end{aligned}
\end{equation*}
and therefore that
\begin{equation}
  \label{eq:naive-update}
  \tilde x_k^T (y - \tilde y_k) = \tilde x_k^T r + \tilde x_k^T \tilde x_k c_k.
\end{equation}

\subsubsection{Caching Reductions}

Observe that \(\tilde x_k\) only changes between subsequent coordinate updates provided that the members of the cluster \(k\) change, for instance if two clusters are merged, a predictor leaves a cluster, or the signs flip (through an update of \(\alpha_k\)).
As a result, it is possible to obtain computational gains by caching \(\tilde x_k\) and \(\tilde x_k^T \tilde x_k\) for each cluster (except the zero cluster, which we do not consider in our coordinate descent step).
When there is no change in the clusters, there is no need to recompute these quantities.
And even when there are changes, we can still reduce the costs involved since \(\tilde x_k\) can be updated in place.
If a large cluster is joined by few new predictors, then the cost of updating may be much lower than recomputing the quantities for the entire cluster.
Also note that, for single-member clusters we only need to store \(\tilde x_k^T \tilde x_k\) since \(\tilde x_k\) is just a column in \(X\) times the corresponding sign.

Letting \(\tilde x_k^\text{old}\) correspond to the value of \(\tilde x_k\) before the update, we note that \(\tilde x_k \gets \tilde x_k^\text{old} + x_j \sign(\beta_j)\) for each \(j \in \mathcal{C}_k^\text{new} \setminus \mathcal{C}_k^\text{old}\) and \(\tilde x_k \gets \tilde x_k^\text{old} - x_j \sign(\beta_j)\) for each \(j \in \mathcal{C}_k^\text{old} \setminus \mathcal{C}_k^\text{new}\).
If only the signs flip, we simply have to also flip the signs in \(\tilde x_k\).

\subsubsection{Covariance Updates}

Notice that we can rewrite the first term in~\eqref{eq:naive-update} as
\begin{equation}
  \begin{aligned}
    \tilde x_k^T r & = \tilde x_k^T y - \sum_{j : \beta_j \neq 0} \tilde x_k^T x_j \beta_j                                                                      \\
                   & = \tilde x_k^T y - \sum_{j : c_j \neq 0} \tilde x_k^T \tilde x_j c_j                                                                       \\
                   & = s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T y - \sum_{j : \beta_j \neq 0} s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T x_j \beta_j                \\
                   & = s_{\mathcal{C}_k}^T {\left(X^T y\right)}_{\mathcal{C}_k} - \sum_{j : \beta_j \neq 0} s_{\mathcal{C}_k}^T X_{\mathcal{C}_k}^T x_j \beta_j \\
                   & = \sum_{j \in \mathcal{C}_k}\left( s_j {x_j}^T y - \sum_{t : \beta_t \neq 0} s_j x_j^T x_t \beta_t \right)
  \end{aligned}
\end{equation}
As in \textcite{friedman2010}, this formulation can be used to achieve so-called \emph{covariance updates}.
We compute \(X^T y\) once at the start.
Then, each time a new predictor becomes non-zero, we compute its inner product with all other predictors, caching these products.


\subsection{Hybrid proximal coordinate descent strategy}

We propose an iterative solver that alternates between proximal gradient step and proximal coordinate descent.
Since the regularization term for SLOPE is not separable, applying PCD does not guarantee convergence (show example where CD gets stuck).
However, \cite{dupuis2021} showed that once the clusters are known, the subdifferential of $J$ can be written as the cartesian product of the subdifferential of $J$ restricted to the clusters.
Hence, if one knew the clusters, PCD updates could be applied on each cluster.

The notion of clusters for SLOPE extends the notion of sparsity coming from the LASSO.
Identification of the sparsity pattern throughout the iterative algorithm have largely been studied.
Talk about Partly Smooth functions, related Manifold and that support transpose to cluster for SLOPE regularization. DO the maths.

Then identification of this underlying structure occurs when applying PGD.
Hence the idea to alternate, PGD and PCD step to take advantage of the speed of PCD and ensure convergence via the identification of the right structure with PGD steps.

\begin{algorithm}[tb]
  \SetKwInOut{Init}{init}
  \SetKwInOut{Input}{input}
  \caption{%
    Hybrid coordinate descent and proximal gradient descent algorithm
    for SLOPE\label{alg:hybrid}}
  \Input{
    \(X \in \mathbb{R}^{n\times p}, y\in \mathbb{R}^n, \beta\in \mathbb{R}^p, \lambda \in \{\mathbb{R}^p : \lambda_1 \geq \lambda_2 \geq \cdots > 0\}\), \(m \in \mathbb{N}\)}

  \Init{\(t \gets 0\), \(\beta \gets 0\), \(L \gets \lVert X \rVert_2^2\)}

  \Repeat{convergence}{
  \(t \gets t + 1\)

  \If{\(t \bmod m = 0\)}{

  \(\beta \leftarrow \operatorname{prox}_{J / L}(\beta - \frac{1}{L}\nabla f(\beta))\) \label{alg:hybrid-istastep}

  % \(C_1, \hdots, C_m \leftarrow \mathtt{get\_clusters}(\beta)\)
  }
  \Else{
    \(k \gets 0\)

    \While{\(k \leq \lvert \mathcal{C} \rvert\)}{
      \(k \gets k + 1\)

      \(s \leftarrow \mathrm{sign}(\beta_{\mathcal{C}_k})\)

      \If{\(s \neq 0\)}{
        % \(L_k \gets (X_{:, \mathcal{C}_k}s)^\top  X_{:, \mathcal{C}_k}s\)


        % \(\tilde{\beta} \gets T(|\beta_{\mathcal{C}_k}| - \frac{1}{L_k} \nabla_{\mathcal{C}_k}f(\beta)^\top s, \frac{\lambda}{L_k})\)

        % \(\beta_{\mathcal{C}_k} \leftarrow \tilde{\beta} s\)
        \(\tilde x_k \gets X_{\mathcal{C}_k}s\)

        \(\tilde y_k \gets X_{\widebar{\mathcal{C}}_k}\beta_{\widebar{\mathcal{C}}_k}\)

        \(\tilde r_k \gets y - \tilde y_k\)

        \(
        \beta_{\mathcal{C}_k} \gets
        s T \left(
        \frac{\tilde r^T_k \tilde x_k, }{ \tilde x_k^T \tilde x_k},
        \frac{\lambda}{ \tilde x_k^T \tilde x_k},
        k,
        \tilde \beta,
        \mathcal{C}
        \right)
        \)
        \Comment{\(\mathcal{C}\) is updated at this step.}


      }

    }
  }

  }
  \Return{\(\beta\)}
\end{algorithm}

\begin{theorem}
  Iterates of \cref{alg:hybrid} converge towards \(\beta^*\).
\end{theorem}
\begin{proof}
  First note that convergence properties of proximal gradient descent on convex
  problems such as SLOPE are well-established
  \parencite{beck2009,daubechies2004}, which certifies that updates via
  \cref{alg:hybrid-istastep} make progress towards \(\beta^*\).

  Next note that the objectives of \eqref{eq:slope-problem} and
  \eqref{eq:subproblem} are equal and that \eqref{eq:subproblem} can be seen
  viewed as a version of \eqref{eq:slope-problem} with added linear constraints
  that is also convex. And, finally, because the coordinate updates of
  \cref{alg:hybrid} minimize the sub-problem, we in the worst case make no
  progress and therefore have guaranteed converge rate no less than \(1/m\)
  of that of proximal gradient descent.
\end{proof}

